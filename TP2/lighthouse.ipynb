{"cells":[{"cell_type":"markdown","metadata":{"id":"W5YyLBrTGVxR"},"source":["# EE-411 Fundamentals of inference and learning, EPFL \n","## Exercice Session 2: optimizing functions using scipy\n","\n","In this second set of exercices, we will solve the so-called *Lighthouse problem*, which serves as a perfect opportunity to introduce a very useful package for scientific computing, called **scipy**.\n","\n","**What you will learn today:** In this second session, we will discuss how to use **scipy** to generate random data according to a certain distribution and to minimize 1D and 2D functions. Furthermore, we will have a first-hand application of some concepts explained in the second lecture, such as the maximal likelihood estimator, Cramér–Rao bound and Jeffreys prior.\n"]},{"cell_type":"markdown","metadata":{"id":"ttGWx3UOCrq4"},"source":["# The Lighthouse problem "]},{"cell_type":"markdown","metadata":{"id":"SdGrctH2HNw3"},"source":["**Locating the lighthouse**\n","\n","A lighthouse is located somewhere off a piece of straight coastline at a position $\\alpha$ (or $x_0$) along the shore and a distance $\\beta$ (or $y_0$) out at sea. It emits a series of short highly collimated flashes at random intervals and hence at random azimuths. These pulses are intercepted on the coast by photo-detectors that record only the fact that a flash has occurred, but not the angle from which it came. N flashes have so far been recorded at positions $\\{x_k\\}$. Where is the lighthouse?’ (from D. Sivia's book, \"Data Analysis - A Bayesian Tutorial\")\n","\n","[![](data:image/jpeg;base64,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)](data:image/jpeg;base64,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)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AstO5OSyDJuQ"},"source":["##### 1) Compute the probability distribution $p(x | x_0,y_0)$ of observing a flash at the point $x$ along the shore depending on the distance of the lighthouse from the shore $y_0$ and its position along the shore $x_0$."]},{"cell_type":"markdown","metadata":{"id":"T464tIWlZOcz"},"source":["#### Recall: Maximum likelihood estimator"]},{"cell_type":"markdown","metadata":{"id":"LrEkdDn3iBGd"},"source":["\n","The maximum likelyhood estimator gives us the parameters of the distribution that make the observation of a given set of data $\\{x_i\\}$ most likely. Assuming that the observations are independent, and that the probability of observing $x$ is given by the probability distribution $P(x|\\lambda)$\n","$$\n","\\widehat{\\lambda}_{ML}=argmax_{\\lambda}\\prod_i P(x_i|\\lambda)=argmax_{\\lambda}\\left(\\sum_i\\log(P(x_i|\\lambda))\\right)\n","$$\n","\n","There are two additional quantities that will be important for our purpuse. The first one is called the score\n","$$\n","S(x,\\lambda)=\\partial_\\lambda L(x|\\lambda),\n","$$\n","where we introduced Log-likelyihood $L(x|\\lambda)=\\log P(x|\\lambda)$. Importantly, the average of $\\mathbb{E}[S(x,\\lambda)]=0$. The second one is the Fisher information, which corresponds to the variance of the score\n","$$\n","I(\\lambda)=\\mathbb{E}[S(x|\\lambda)^2].\n","$$\n","In case of $N$ independent events we simply have\n","$$\n","I_N(\\lambda)=\\sum_{i=1}^N I(\\lambda) = NI(\\lambda).\n","$$"]},{"cell_type":"markdown","metadata":{"id":"KXVFutIpm_ym"},"source":["### Consider the case with $x_0=20$, $y_0=15$."]},{"cell_type":"markdown","metadata":{"id":"bFkSuuZxlBoi"},"source":["#### Let us first assume that we know the value of $x_0$ but not the one of $y_0$. "]},{"cell_type":"markdown","metadata":{"id":"u1Ln1tvPIO1c"},"source":["Then, we wish to find\n","\n","$$p(y_0 | \\{x_k\\}, x_0)$$\n","\n","Using Bayes’ theorem:\n","\n","$$p(y_0 | \\{x_k\\}, x_0) = \\frac{p(y_0 , \\{x_k\\}, x_0)}{p(\\{x_k\\}, x_0)} = \\frac{p(\\{x_k\\} | y_0, x_0)p(y_0 | x_0)p(x_0)}{p(\\{x_k\\} | x_0)p(x_0)}\\propto p(\\{x_k\\} | y_0, x_0) p(y_0)$$\n","\n","where the last step comes from the fact that  $y_0 \\perp x_0$, and so we have $p(y_0 | x_0) = p(y_0)$.\n","The most intuitive prior $p(y_0)$ when we don't have additional information is the one which is uniform over a large enough segment $[y_{min}, y_{max}]$, i.e. $p(y_0) = 1/(y_{max}-y_{min})$\n","\n","Even if this seems to be a good choice in general, we will show that there exist other priors that lead to a better result.\n","For the data likelihood $p(\\{x_k\\} | y_0, x_0)$ we consider that each datum $x_k$ is i.i.d., so:\n","\n","$$p(\\{x_k\\} | y_0, x_0) = \\prod_{k=1}^N p(x_k | y_0, x_0)$$\n","\n","And since the prior for $y_0$ is uniform, we finally get:\n","\n","$$p(y_0 | \\{x_k\\}, x_0) \\propto \\prod_{k=1}^N p(x_k | y_0, x_0)$$\n","\n","Therefore, considering a uniform prior, maximizing the posterior coincides with maximizing the Log-likelihood $L(\\{x_k\\} | x_0,y_0)$ "]},{"cell_type":"markdown","metadata":{"id":"uecSDeC2hrsN"},"source":["##### We now make an experiment, by generating $N=100$ samples and plotting the dependence of the Log-likelyhood as a function of $y_0$. "]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1632652467577,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"EN2bkJ36hrsk"},"outputs":[],"source":["import numpy as np\n","np.random.seed(123456) # we set the seed of the random generator"]},{"cell_type":"markdown","metadata":{"id":"Wd_PMT9Chrsl"},"source":["First we define the function `loglh` to compute the \n","log-likelihood for a set of $N$ instances $x$, given the parameters $x_0$ and $y_0$."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1632652474881,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"UIfz-3M0hrsl"},"outputs":[],"source":["def loglh(x0,y0,x):\n","  logl=0\n","  for i in x: logl+=np.log(y0/(y0**2+(i-x0)**2)/np.pi)\n","  return logl"]},{"cell_type":"markdown","metadata":{"id":"GRlkFBoKhrsl"},"source":["To generate the set of observations $\\{x_k\\}$, since we know they follow a Cauchy distribution, we can use a built-in function of the `scipy` package"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1099,"status":"ok","timestamp":1632652568770,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"ZdjPENychrsl"},"outputs":[],"source":["from scipy.stats import cauchy\n","\n","N=100\n","x0_true=20\n","y0_true=15\n","\n","x=cauchy.rvs(loc=x0_true,scale=y0_true,size=N) # rvs stands for Random Variates, i.e. particular outcomes of a random variable"]},{"cell_type":"markdown","metadata":{"id":"Hzk-bF8ghrsl"},"source":["Finally we can compute the Log-likelihood for a range of values of $y_0$ around the true value $y_0^*$ and plot its behaviour"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"elapsed":2334,"status":"ok","timestamp":1632652880641,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"38oyKLVchrsm","outputId":"e55d7191-62cd-497e-cbb8-aecbf7192a55"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","tab_y0=np.linspace(0,30,200)[1:] # we generate a table for y0 \n","logl_y0=np.array([loglh(x0_true,y0,x) for y0 in tab_y0]) # we compute the loglh for each value in the table\n","\n","fig = plt.figure(figsize=(9,6), dpi=80)\n","plt.title('Log-likelihood', fontsize=16)\n","plt.xlabel('$y_0$', fontsize=14)\n","plt.ylabel('$L(y_0)$', fontsize=14)\n","plt.plot(tab_y0,logl_y0)\n","plt.axvline(y0_true,color=\"r\",label='$y_0*$')\n","plt.legend(fontsize=14);"]},{"cell_type":"markdown","metadata":{"id":"Vl9oyg1Shrsm"},"source":["We see that there’s a maximum near the true value, but it's not easy to see since we are working with logs.\n","\n","However, if we compute the exponential we can get the shape of the posterior $p(y_0 | \\{x_k\\}, 20)$ (to get the true posterior – a distribution – we would need to find the normalizing factor, which is irrelevant here)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"elapsed":549,"status":"ok","timestamp":1632652883159,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"MK7F0m6whrsm","outputId":"f00513a9-52ce-4126-8bbd-63f42826c696"},"outputs":[],"source":["fig = plt.figure(figsize=(9,6), dpi=80)\n","plt.title('Log-likelihood', fontsize=16)\n","plt.xlabel('$y_0$', fontsize=14)\n","plt.ylabel('$e^{(L-L_{\\max})}$', fontsize=14)\n","plt.plot(tab_y0,np.exp(logl_y0-max(logl_y0)))\n","plt.axvline(y0_true,color=\"r\",label='$y_0*$')\n","plt.legend(fontsize=14);"]},{"cell_type":"markdown","metadata":{"id":"c4tBWmFrmgM0"},"source":["##### Now we compute the maximum likelihood estimator for $y_0$, that we call $\\widehat{y}_{0,ML}$, and we see how it behaves when we vary the number of data in the sample $x$"]},{"cell_type":"markdown","metadata":{"id":"gZnl2HC0jEW_"},"source":["A good way to compute the maximum of a function is to use a function from `scipy.optimize` called `minimize`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5517,"status":"ok","timestamp":1632653143672,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"ld0YrPSPn7PC","outputId":"a134b884-b9e0-42db-9c67-b944ff513976"},"outputs":[],"source":["%%time\n","from scipy.optimize import minimize\n","\n","tab_N = np.logspace(0, 4, num=10, base=10)\n","y0_ML=[]\n","for N in tab_N:\n","  x=cauchy.rvs(loc=x0_true,scale=y0_true,size=int(N))\n","  y0_guess = np.random.uniform(0,30) # we have to give a guess for the minimizer algorithm to start\n","  optimum = minimize(lambda y0:-loglh(x0_true,y0,x),y0_guess, method='Nelder-Mead')\n","  y0_ML.append(optimum.x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1632653144204,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"JZP5awMCi-Ku","outputId":"e6ab4764-60d4-4081-cc5c-371bb0315a60"},"outputs":[],"source":["fig = plt.figure(figsize=(9,6), dpi=80)\n","plt.title('$\\widehat{y}_{0,ML}$', fontsize=16)\n","plt.xlabel('$N$', fontsize=14)\n","plt.plot(tab_N,y0_ML,\"*\",color='r')\n","plt.axhline(y0_true, label=\"$y_0*$\")\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","plt.legend(fontsize=14);"]},{"cell_type":"markdown","metadata":{"id":"-na8Lpd_hwoT"},"source":["#### 2) Now do the same, but in the case in which we know $y_0 = 15$ but we do not know $x_0$, that is again $20$:\n","* Generate $N=100$ samples Cauchy-distributed according to the same parameters $x_0$ and $y_0$\n","* Plot  the Log-likelihood for a range of values of $x_0$ around the true value $x_0^*$\n","* Compute the maximum likelihood estimator for $x_0$ , that we call $\\widehat{x}_0$ , and see how it behaves for $N\\in[1,10^4]$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m3gFa-h7n7vM"},"source":["#### 3) Now, after having generated data for N in $[1,10^7]$, try to compute the mean $\\left<x\\right>$ for each sample as a function of $N$. What do you observe? What about the median?"]},{"cell_type":"markdown","metadata":{"id":"reHrcItKwBCF"},"source":["## MSE: which estimator is the best?"]},{"cell_type":"markdown","metadata":{"id":"xGYXtw60wcgU"},"source":["First of all, let's recall the definition:\n","\n","**Mean Squared Error**: The mean square error (MSE) is the expectation\n","value of the square of the difference beween the estimator and the true value of parameter.\n","\n","$$MSE(\\widehat{\\lambda};\\lambda) \\equiv \\mathop{\\mathbb{E}_{\\widehat{\\lambda}}}\\big[ (\\widehat{\\lambda} - \\lambda)^2 \\big]$$"]},{"cell_type":"markdown","metadata":{"id":"03XSYGcr0ZbZ"},"source":["#### **FISHER INFORMATION**"]},{"cell_type":"markdown","metadata":{"id":"0QO8vcUy0hvI"},"source":["As we recalled earlier, for $N$ independent events we have\n","$$\n","I_N(\\lambda)=\\sum_{i=1}^N I(\\lambda) = NI(\\lambda).\n","$$\n","\n","Therefore for our problem\n","\n","$$\n","I_N(x_0, y_0) = N \\mathbb{E}_x[S(x|x_0)^2] = -N \\mathbb{E}_x\\begin{bmatrix}\\frac{\\partial^2 L}{\\partial x_0^2} & \\frac{\\partial^2 L}{\\partial x_0\\partial y_0}\\\\ \\frac{\\partial^2 L}{\\partial y_0\\partial x_0} & \\frac{\\partial^2 L}{\\partial y_0^2}\\end{bmatrix}\n","$$\n","\n","And after a bit of computations, and a little help from [wolfram alpha](https://www.wolframalpha.com/calculators/integral-calculator/), one finds that if $x \\in(-\\infty,+\\infty)$ then the Fisher information is\n","\n","$$I_N(x_0, y_0) =\\begin{bmatrix}\\frac{N}{2y_0^2} & 0\\\\0 & \\frac{N}{2y_0^2}\\end{bmatrix} = \\frac{N}{2y_0^2} \\mathbb{I}_2$$\n","\n","And therefore\n","\n","*   $\\det [I_N(x_0,y_0)] = \\frac{N^2}{4y_0^4}$\n","*   $I_N(y_0|x_0) = \\frac{N}{2y_0^2}$\n","*   $I_N(x_0|y_0) = \\frac{N}{2y_0^2} = \\text{const}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EMuCKvx2nFGU"},"source":["#### **CRAMÉR-RAO BOUND**"]},{"cell_type":"markdown","metadata":{"id":"yhQKwYTdns75"},"source":["For **unbiased estimators**, as for example the maximum likelihood one, the CRB tells us that for each parameter $\\lambda_i$ we have\n","\n","$$MSE(\\widehat{\\lambda_i}_{ML};\\lambda_i) \\geq \\{[I_N(\\mathbf{\\lambda})]^{-1}\\}_{ii}$$\n","\n","where $I_N(\\mathbf{\\lambda})$ is the Fisher information."]},{"cell_type":"markdown","metadata":{"id":"vRPqYw-A5O8q"},"source":["#### **JEFFREYS PRIOR**"]},{"cell_type":"markdown","metadata":{"id":"Be80xIMgyBME"},"source":["In general, the Jeffreys prior is given by\n","\n","$$p(\\mathbf{\\lambda}) \\propto \\sqrt{\\det[I_N(\\mathbf{\\lambda})]}$$"]},{"cell_type":"markdown","metadata":{"id":"FnZUt2eCN3UT"},"source":["#### Consider the case in which $x_0=20$, and we know its value, and we want to estimate $y_0$"]},{"cell_type":"markdown","metadata":{"id":"_RInpUfDVxiy"},"source":["In this case the Cramér-Rao bound tells us:\n","\n","$$MSE(\\widehat{y_0}_{ML};y_0) \\geq \\frac{2y_0^2}{N}$$\n","\n","While the Jeffreys prior, normalized, is $p(y_0) = \\frac{1}{y_0}$ and thus the posterior \n","\n","$$p(y_0 | \\{x_k\\}, x_0) \\propto p(\\{x_k\\} | y_0, x_0) p(y_0) \\propto \\frac{1}{y_0}\\prod_{i=1}^{N}  \\frac{1}{\\pi} \\frac{y_0}{(x_i-x_0)^2 + y_0^2}$$"]},{"cell_type":"markdown","metadata":{"id":"vUXn7SC1W5U3"},"source":["Now let's compute the MSE on the likelihood estimator and on the one found with Jeffreys prior, and compare the two with the Cramér-Rao bound"]},{"cell_type":"markdown","metadata":{"id":"qKWi-I7Ay_kf"},"source":["Firstly, we define the function to compute the posterior with Jeffreys prior"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1632654323365,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"BNUn0sPHy_QJ"},"outputs":[],"source":["def jeffpost(x0,y0,x):\n","  jp= - np.log(y0) #prior\n","  for i in x: jp+=np.log(y0/((y0**2+(i-x0)**2)*np.pi))\n","  return jp"]},{"cell_type":"markdown","metadata":{"id":"1JPn8Bgxx_Kg"},"source":["Since we are going to use it a lot of times, we define a new function `minimizer_y0` to compute the estimators given a function to minimize  "]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":514,"status":"ok","timestamp":1632654357668,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"trp1LlpSIWIt"},"outputs":[],"source":["def minimizer_y0(func,y0):\n","  y0_est=np.zeros(N_rep)\n","  for ir in range(N_rep):\n","    x=cauchy.rvs(loc=x0_true,scale=y0,size=N)\n","    y0_est[ir] = minimize(lambda y:-func(x0_true,y,x),np.random.uniform(0,30), method='Nelder-Mead').x\n","  return y0_est;"]},{"cell_type":"markdown","metadata":{"id":"azlkTN8_9MC0"},"source":["To estimate the $MSE$, i.e. to compute the expected value over $\\widehat{\\lambda}$, we need to repeat the estimation $N_{rep}$ times. We will do this for $N_{est} = 15$ values of $y_0$ in $(0,1]$, starting with $N=10$ and $N_{rep} = 500$"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":353,"status":"ok","timestamp":1632654495106,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"9jOuSVYN24gN"},"outputs":[],"source":["N_est=15\n","tab_y0=np.linspace(0,1,N_est+1)[1:]\n","N = 10\n","N_rep = 500"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79624,"status":"ok","timestamp":1632654576562,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"mnrafArWnGId","outputId":"4511f2b9-3aa0-4ba6-d3c1-c3a70454aaf0"},"outputs":[],"source":["%%time\n","\n","y0_ML= np.array([ minimizer_y0(loglh,y0) for y0 in tab_y0])\n","y0_J= np.array([ minimizer_y0(jeffpost,y0) for y0 in tab_y0])"]},{"cell_type":"markdown","metadata":{"id":"Uu4oGTxl_xpM"},"source":["Now that we have computed $N_{rep}$ estimates for each value of $y_0$ with both methods, we can estimate the $MSE$ and we can use **bootstrap** to give a confidence interval on our estimations"]},{"cell_type":"markdown","metadata":{"id":"CrWXpXrMA-Vb"},"source":["First we define the function `MSE_est`\n","that, given the array of $N_{rep}$ estimates of $y_0$, returns the estimate of the $MSE$ and its C.I."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":375,"status":"ok","timestamp":1632654701230,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"LvX7CsbAGVbf"},"outputs":[],"source":["from numpy import percentile\n","from numpy.random import choice\n","\n","def MSE_est(y0_est,y0):\n","  N_boot=100\n","  MSE = (y0_est - np.full(N_rep, y0))**2 #avoid for loops!\n","  MSE_mean = MSE.mean()\n","  MSE_boot = choice(MSE,(N_boot,N_rep))\n","  MSE_boot_mean = MSE_boot.mean(axis=1)\n","  CI_min = percentile(MSE_boot_mean, [2.5,97.5])[0]\n","  CI_max = percentile(MSE_boot_mean, [2.5,97.5])[1]\n","\n","  return MSE_mean, CI_min, CI_max"]},{"cell_type":"markdown","metadata":{"id":"GsAcxpMnEwx9"},"source":["And now we can compute our estimates for each $y_0$"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1632654772559,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"wsRvIGTa8uSc"},"outputs":[],"source":["MSE_ML_10 = np.zeros(N_est)\n","CI_min_ML_10 = np.zeros(N_est)\n","CI_max_ML_10 = np.zeros(N_est)\n","MSE_J_10 = np.zeros(N_est)\n","CI_min_J_10 = np.zeros(N_est)\n","CI_max_J_10 = np.zeros(N_est)\n","\n","for j,y0 in enumerate(tab_y0):\n","  MSE, CI_min, CI_max = MSE_est(y0_ML[j], y0)\n","  MSE_ML_10[j] = MSE\n","  CI_min_ML_10[j] = CI_min\n","  CI_max_ML_10[j] = CI_max\n","  MSE, CI_min, CI_max = MSE_est(y0_J[j], y0)\n","  MSE_J_10[j] = MSE\n","  CI_min_J_10[j] = CI_min\n","  CI_max_J_10[j] = CI_max"]},{"cell_type":"markdown","metadata":{"id":"S56ZNLisE7SF"},"source":["And compare them to the Cramér-Rao bound"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1632654778417,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"zx2dxTAWy-VR","outputId":"8e725aae-7b93-4935-e3b2-05e1cc09d414"},"outputs":[],"source":["fig = plt.figure(figsize=(9, 6), dpi=80)\n","\n","plt.title('MSE', fontsize=16)\n","plt.xlabel('$y_0$', fontsize=14)\n","plt.errorbar(tab_y0,MSE_J_10,yerr=[MSE_J_10-CI_min_J_10, CI_max_J_10-MSE_J_10],marker=\"\",color='g', label='Jeffrey')\n","plt.errorbar(tab_y0,MSE_ML_10,yerr=[MSE_ML_10-CI_min_ML_10, CI_max_ML_10-MSE_ML_10],marker=\"\",color='r', label='ML')\n","plt.plot(tab_y0, 2*tab_y0*tab_y0/N, label='Cramers-Rao')\n","plt.legend(fontsize=14)"]},{"cell_type":"markdown","metadata":{"id":"BfDm2J6VnjEV"},"source":["#### 4) Now do the same for $N=3$ and $N=50$ and plot all in a single graph, to see what happens when we vary $N$"]},{"cell_type":"markdown","metadata":{"id":"7RcU0rSgYutu"},"source":["#### 5) Now consider the case in which we know the value of $y_0=15$ but not $x_0$:\n","\n","* What Jeffreys prior tells us in this case?\n","* Compare the MSE on the maximum likelihood and the median estimators for values of $x_0$ in $[-1,1]$ and compare both to the Cramér-Rao bound\n","* Repeat the process for $N=3,50$ as before and compare the results"]},{"cell_type":"markdown","metadata":{"id":"9igUrErnHtxn"},"source":["## Estimate both the parameters\n","Now we consider again the case with $y_0=15$, $x_0=20$, but let's assume that we do not know either $x_0$, $y_0$. "]},{"cell_type":"markdown","metadata":{"id":"AfKw78YiJqdO"},"source":["If we assume that both $x_0$ and $y_0$ are uniform over some interval, then the posterior is simply proportional to the likelihood:\n","$$p(x_0, y_0 | \\{x_k\\}) \\propto p(\\{x_k\\} | x_0, y_0)$$"]},{"cell_type":"markdown","metadata":{"id":"mshPuKRnKM9H"},"source":["We make an experiment, by generating $N=100$ samples and we plot the dependence of log-likelyhood as a function of $x_0$ and $y_0$. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1668,"status":"ok","timestamp":1632656222223,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"eCzH_-IuKMwm","outputId":"43ea6a8b-9740-4ec6-9c48-648bcd8fc4a7"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","x0_true=20\n","y0_true=15\n","N=100\n","tab_y0=np.linspace(0,30,201)[1:]\n","tab_x0=np.linspace(0,40,200)\n","x=cauchy.rvs(loc=x0_true,scale=y0_true,size=N)\n","#logl=[[loglh(x0,y,x) for y in taby0] for x0 in tabx0]\n","\n","fig = plt.figure(figsize=(9,6), dpi=80)\n","ax = fig.add_subplot(111, projection='3d') #we create the 3d axis\n","plt.title('Log-likelihood', fontsize=16)\n","X, Y = np.meshgrid(tab_x0,tab_y0)\n","zs = np.array(loglh(np.ravel(X), np.ravel(Y),x))\n","Z = zs.reshape(X.shape)\n","\n","ax.plot_surface(X, Y, Z, alpha=0.8)\n","ax.plot( np.full(1000, x0_true), np.full(1000, y0_true), np.linspace(max(zs)+200, min(zs), 1000))\n","\n","ax.set_xlabel('$x_0$', fontsize=14)\n","ax.set_ylabel('$y_0$', fontsize=14)\n","ax.set_zlabel('$L$', fontsize=14)\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":544},"executionInfo":{"elapsed":1792,"status":"ok","timestamp":1632656252287,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"wyY3y_qkH23J","outputId":"7379af84-174d-4528-c140-9fec84811ba1"},"outputs":[],"source":["fig = plt.figure(figsize=(12, 10), dpi=80)\n","ax = fig.add_subplot(111, projection='3d')\n","plt.title('Log-likelihood', fontsize=16)\n","X, Y = np.meshgrid(tab_x0,tab_y0)\n","zslog = np.array(np.exp(loglh(np.ravel(X), np.ravel(Y),x) - max(zs)))\n","Z = zslog.reshape(X.shape)\n","\n","ax.plot_surface(X, Y, Z, alpha=0.8)\n","ax.plot( np.full(1000, x0_true), np.full(1000, y0_true), np.linspace(0,1, 1000),color='r')\n","\n","ax.set_xlabel('$x_0$', fontsize=14)\n","ax.set_ylabel('$y_0$', fontsize=14)\n","\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SoUIL3ar5gJY"},"source":["##### And finally we compute the ML estimator of both the parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8338,"status":"ok","timestamp":1632656414687,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"yAezZPKrgwqs","outputId":"345a3d27-3018-43a3-ab3e-239d9960ea20"},"outputs":[],"source":["%%time\n","tab_N= np.logspace(0, 4, base=10, num=20)\n","y0_ML=[]\n","x0_ML=[]\n","\n","def f(params):\n","  x0,y0 = params\n","  return -loglh(x0,y0,x)\n","\n","for N in tab_N:\n","  x=cauchy.rvs(loc=x0_true,scale=y0_true,size=int(N))\n","  xy0 = (np.random.uniform(0,40),np.random.uniform(0,30))\n","  #bounds = [(-40,40),(0,50)]\n","  optimum = minimize(f,xy0, method='Nelder-Mead')\n","  x0_ML.append(optimum.x[0])\n","  y0_ML.append(optimum.x[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"elapsed":947,"status":"ok","timestamp":1632656554189,"user":{"displayName":"davide ghio","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16033550004002351519"},"user_tz":-120},"id":"GeJkb2r0kGRM","outputId":"0521f163-c340-4990-b972-6e205b763275"},"outputs":[],"source":["fig = plt.figure(figsize=(12, 10), dpi=80)\n","ax = fig.add_subplot(111, projection='3d')\n","sizes = np.array([int(N) for N in tab_N])\n","print(len(sizes),len(x0_ML), len(y0_ML))\n","plt.title('ML-estimates', fontsize=16)\n","ax.plot( range(10000), np.full(10000, x0_true), np.full(10000, y0_true),color='r')\n","ax.plot(tab_N, x0_ML, y0_ML, marker=\"*\",color='b', ls='None')\n","ax.set_xlabel('$N$', fontsize=14)\n","ax.set_ylabel('$x_0$', fontsize=14)\n","ax.set_zlabel('$y_0$', fontsize=14)\n","\n","ax.set_ylim(-50,50)\n","ax.set_zlim(0,30)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QjWkRROH60fr"},"source":["## Evaluated!\n","\n","#### 6) Take the dataset you find in `where_is_the_light.npy` file and import it into a numpy array. It contains $N=10000$ Cauchy-distributed samples with unknown parameters $x_0$ and $y_0$. Using the techniques previously showed, give an estimate (with a confidence interval) of the true value of $x_0$ and $y_0$. You are guaranteed that $x_0 \\in [-100,100]$ and $y_0 \\in [0,30]$. *Bonus*: which one is better between Jefferys prior and uniform prior?"]}],"metadata":{"colab":{"collapsed_sections":["AstO5OSyDJuQ","-na8Lpd_hwoT","m3gFa-h7n7vM","BfDm2J6VnjEV","7RcU0rSgYutu","QjWkRROH60fr"],"name":"FoIL_ex2.ipynb","provenance":[{"file_id":"1m2YBrQLfhVSMBpTEsfvOf4hXVZplp1D7","timestamp":1631517725958}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
