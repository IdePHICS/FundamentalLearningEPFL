{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ua2GXXdLAOM"
      },
      "source": [
        "# Kernel regression\n",
        "\n",
        "**What you will learn today**: You will learn how to implement kernel regression. First we analyze a synthetic dataset: you will build the feature map associated with different kernels and we see how the learning performance can be different. As always after having understood the theoretical insights we step on to real-dataset problem: you can play using our dear friend sklearn to implement Kernel Ridge Regression, with a kernel of your choice.\n",
        "\n",
        "**Important note!** There are 2 evaluated questions! Make sure to answer them to get the points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO01wTd8YXis"
      },
      "source": [
        "# 1) KRR from scratch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfdFiHpoG1sM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmvMeORHi32A"
      },
      "source": [
        "Remember first lecture?  Let us use pandas for handling synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-TWxMiSU7A8"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dataTP8.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jycsJaAU7BG"
      },
      "outputs": [],
      "source": [
        "x,y = np.array(data['X']), np.array(data['Y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scs8ggEji-j5"
      },
      "source": [
        "Let's see how it looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kczXc9wSjAxk",
        "outputId": "452f8b37-7e50-4128-a915-dde5db0b46e4"
      },
      "outputs": [],
      "source": [
        "plt.plot(x,y,'o') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TWEFxamjWmc"
      },
      "source": [
        "It does not seem a bad idea to use a polynomial fitting here. We have introduced in the theoretical lectures kernel methods: $$K(x,y) = {\\phi(x)}^T{\\phi(y)}$$\n",
        "\n",
        "Let us pick for the first part of the analysis a feature map of the form:\n",
        "$$ \\phi_k^{(p)}(x) = x^k  \\,\\,\\,\\,\\,\\,\\,\\, k=1 \\dots p $$\n",
        "\n",
        "You may have studied this under the name *linear basis regression*.\n",
        "\n",
        "The tools we need once we have mapped the datapoints in feature space are the usual one of linear regression as we know and all will be straight-forward.\n",
        "We will study the behaviour as the degree $p$ varies, this will vary the complexity of the fitting model leading us to our usual bias-variance tradeoff consideration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNYMZE6eUEsR"
      },
      "outputs": [],
      "source": [
        "def build_poly(x, degree,choice='none',gamma=1):\n",
        "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
        "    x_pol = []\n",
        "    \n",
        "    for i in range(0,len(x)):\n",
        "        x_pol_ele = []\n",
        "        if choice == \"rbf\":\n",
        "          x_pol_ele = np.exp(-x[i]*x[i]/gamma)*np.array([x[i]**j/(np.math.factorial(j)*np.math.sqrt(gamma**j)) for j in range(0,degree+1)])\n",
        "        else:\n",
        "          x_pol_ele = np.array([x[i]**j for j in range(0,degree+1)])\n",
        "        x_pol.append(x_pol_ele)\n",
        "    return np.array(x_pol)\n",
        "\n",
        "def ridge_regression(y, X, lambda_):\n",
        "    \"\"\"implement ridge regression.\"\"\"\n",
        "    Xt = np.transpose(X)\n",
        "    I = np.eye(len(Xt))\n",
        "    w =  np.linalg.solve(Xt@X + (2*lambda_*len(y))*I,Xt@y)\n",
        "    mse = (1/(2*len(y)))*np.sum((y-np.matmul(X,w))**2)\n",
        "    return mse,w\n",
        "\n",
        "def polynomial_regression(x,y,degrees,lambda_):\n",
        "    num = len(degrees)\n",
        "    fig, axs = plt.subplots(num,figsize=(15,15)) ; i =-1\n",
        "    for degree in (degrees):\n",
        "        i+=1   ; phi = build_poly(x, degree)\n",
        "        \n",
        "        mse , weights = ridge_regression(y, phi,lambda_)\n",
        "        rmse = np.sqrt(2*mse)\n",
        "\n",
        "        print(\"Processing {j}th experiment, degree={d}, rmse={loss}\".format(\n",
        "              j=i + 1, d=degree, loss=rmse))\n",
        "        # print(weights)\n",
        "        # plot fit\n",
        "        # fhat = np.dot(phi,weights)\n",
        "        xvals = np.arange(min(x) - 0.1, max(x) + 0.1, 0.1)\n",
        "        phi = build_poly(xvals, degree) ; fhat = np.dot(phi,weights)\n",
        "        axs[i].plot(x,\n",
        "            y, 'o',label=f'Degree={degree} -- Data')\n",
        "        axs[i].plot(xvals,\n",
        "            fhat,label=f'Fit')\n",
        "        axs[i].set_ylim([-1.4,1.4])\n",
        "        axs[i].legend()\n",
        "    # plt.tight_layout()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHGecKCgUsc8"
      },
      "outputs": [],
      "source": [
        "degrees = [5,  12, 20,50,80,120,160] ; lambda_ = 10e-6\n",
        "polynomial_regression(x,y,degrees,lambda_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SCfjIpFnfkz"
      },
      "source": [
        "We clearly see that by choosing a value of the degree too high we start to have bad fitting (i.e. overfitting). This feature map is not building a good kernel for studying the data.\n",
        "\n",
        "It is interesting to understand if we can build another feature map, always in a polynomial fashion, that is able to having good learning rates.\n",
        "\n",
        "First let's introduce some functions to split data in training and testing dataset to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoNBUtWzbOiu"
      },
      "outputs": [],
      "source": [
        "def split_data(x, y, ratio, seed=1):\n",
        "    # set seed\n",
        "    np.random.seed(seed)\n",
        "    # ***************************************************\n",
        "    index = np.arange(0,len(x))\n",
        "    index_train = np.random.choice(index, int(ratio*len(x)), replace=False)\n",
        "    \n",
        "    training_x = x[index_train]\n",
        "    training_y = y[index_train]\n",
        "    test_x = x[[i for i in range(len(x)) if i not in index_train]]\n",
        "    test_y = y[[i for i in range(len(y)) if i not in index_train]]\n",
        "    \n",
        "    return training_x, training_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYNq5-Z0oiGm"
      },
      "source": [
        "## Exercise\n",
        "* Implement kernel regression with the feature map parametrized by $\\gamma$ which we encode with the codework *rbf*: \n",
        "\n",
        "$$ \\phi_k^{(p)}(x) = e^{\\frac{-x^2}{\\gamma}}\\frac{x^k}{k!}  \\,\\,\\,\\,\\,\\,\\,\\, k=1 \\dots p $$\n",
        "* Study the behaviour as a function of $\\lambda$ of the learning curves (test error and training error) and compare with what you would see with the previous feature map.\n",
        "* What do you conclude?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH_m9SOc0-Vg"
      },
      "source": [
        "Let us implement first the function for finding the learning curves of KRR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ridge_regression_learning_curves(x, y, degree, ratio,lambdas, seed,choice , gamma):\n",
        "    \"\"\"ridge regression demo.\"\"\"\n",
        "    # define parameter\n",
        "    training_x,training_y,test_x,test_y = split_data(x, y, ratio, seed)\n",
        "\n",
        "    phi_train = build_poly(training_x, degree,choice,gamma)\n",
        "    phi_test = build_poly(test_x, degree,choice,gamma)\n",
        "\n",
        "    rmse_tr = []\n",
        "    rmse_te = []\n",
        "    for _, lambda_ in enumerate(lambdas):\n",
        "        mse_train,w = ridge_regression(training_y, phi_train, lambda_)\n",
        "        rmse_tr.append(np.sqrt(2*mse_train))\n",
        "        \n",
        "        mse_test = (1/(2*len(test_y)))*np.sum((test_y-np.matmul(phi_test,w))**2)\n",
        "        rmse_te.append(np.sqrt(2*mse_test))\n",
        "    \n",
        "        # print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
        "        #        p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
        "\n",
        "    return rmse_tr , rmse_te"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us start with the previous feature map, we already saw that the fitting was not good at all when the degree was too high (i.e. overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 1926  ; split_ratio = 0.5   ; choice = 'none'; lambdas = np.logspace(-6, 1, 100)\n",
        "fig, ax = plt.subplots(len(degrees), figsize = (15,15)) \n",
        "for i,degree in enumerate(degrees):\n",
        "  a,b = ridge_regression_learning_curves(x, y, degree, split_ratio, lambdas,seed,choice , 1)\n",
        "  ax[i].plot(lambdas,a,label=f\"Training, degree ={degree}\") ;  ax[i].plot(lambdas,b,label=f\"Test, degree={degree}\") \n",
        "  ax[i].legend() ; ax[i].set_xscale('log') ; ax[i].set_yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IlWqK3VYgKX"
      },
      "source": [
        "# 2) Kernel methods using Sklearn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcGgKgZrfuD"
      },
      "source": [
        "We have seen how to build Kernel Ridge Regression (KRR) from scratch starting from the feature map. The idea to map datapoint in an higher dimensional space (i.e. kernel trick) can be used also in classification task. We will consider again our old friend MNIST, we will consider SVC (Support Vector Classification). We will see that, at stake with previous lecture, *SVC* will accept the parameter *kernel* to choose in which space mapping our datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOdvFJG4tu27",
        "outputId": "6b8f3447-bb58-42ff-fc84-b61ee4c44db0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from sklearn.datasets import fetch_openml # MNIST data\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# Turn down for faster convergence\n",
        "t0 = time.time()\n",
        "train_size = 9000 # 60000\n",
        "test_size = 1500 # 10000\n",
        "\n",
        "### load MNIST data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
        "X = np.array(X)[:train_size+test_size]\n",
        "y = np.array(y)[:train_size+test_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKDGn265Uf_4"
      },
      "source": [
        "Let's plot an image to see how it looks like with plt.imshow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "PP8PLOdX4tff",
        "outputId": "f5ed26f5-c2d7-4319-e17b-0ee730d27ef6"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[6542,:].reshape(28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkGXOWzXdTN"
      },
      "source": [
        "We shuffle the data and we do the test-train splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClqVsH714o5W"
      },
      "outputs": [],
      "source": [
        "# shuffle data\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "# pick training and test data sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=train_size,test_size=test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZZr4RxTvbHW"
      },
      "source": [
        "### Exercise [EVALUATED 1]\n",
        "Use `sklearn.svm.SVC` to classify the MNIST dataset. Use the following kernels: `linear`, `poly`, `rbf`, `sigmoid`. For each kernel do a cross-validation to find the best hyperparameters. Use `GridSearchCV` to do that.\n",
        "Finally return the best global model and the best kernel for the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backpropagation from scratch\n",
        "\n",
        "**What will you learn today**: Backpropagation is the fundamental building block of deep learning, as it allows to automatically compute the gradients of any arbitrary compositional function with a similar computational cost than evaluating it. This makes possible to work with arbitrarily complex neural network architectures, composed of many layers, without the need to manually compute their gradients. Backpropagation is already implemented in all high-level deep learning frameworks, e.g. `PyTorch`, and as such, we would hardly ever need to think of how it works. However, it is a very educational exercise to implement it once in your life, and that is precisely what we will do in this exercise! In particular, you will learn to implement and derive the forward and backward pass of a very simple neural network in pure `numpy`. As a bonus, we will also explore how to approximate the non-convex loss landscape of a neural network by a convex one, and we will learn how to use such approximation to derive intuitions about how different design choices affect the network's behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1) Forward pass\n",
        "To simplify the exercise, we will only work with a simple architecture, consisting of a feedforward neural network with two fully connected layers, i.e., a single-hidden layer MLP.\n",
        "\n",
        "![simple_mlp](simple_mlp.png)\n",
        "\n",
        "Mathematically, we can write the feedforward computation as:\n",
        "$$ x_j^{(1)}=\\sigma\\left(z_j^{(1)}\\right)=\\sigma\\left(\\sum_{i=1}^D w_{i,j}^{(1)} x_i^{(0)}+b_j^{(1)}\\right), $$\n",
        "$$ \\hat y =\\sigma\\left(z_1^{(2)}\\right)=\\sigma\\left(\\sum_{i=1}^K w_{i,1}^{(2)} x_i^{(1)}+b_1^{(2)}\\right),  $$\n",
        "where $\\sigma(\\cdot)$ denotes the sigmoid activation function. In the rest of the exercise, we will use $D=4$, and $K=5$.\n",
        "\n",
        "We can alternatively write the same computation in vector notation\n",
        "$$ \\bf x^{(1)}=\\sigma\\left(\\bf z^{(1)}\\right)=\\sigma\\left(\\bf W^{(1)} \\bf x^{(0)}+\\bf b^{(1)}\\right), $$\n",
        "$$ \\hat y=\\sigma\\left(z^{(2)}\\right)=\\sigma\\left(\\bf {w^{(2)}}^\\top \\bf x^{(1)}+b^{(2)}\\right). $$\n",
        "\n",
        "In general, we will denote the function computed by the neural network as $f_{\\bf w}(\\bf x)=\\hat y$, and use $\\bf w$ to represent the vector of all weights in the architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a warm up, let's implement the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(t):\n",
        "    \"\"\"apply sigmoid function on t.\"\"\"\n",
        "    return 1.0 / (1 + np.exp(-t))\n",
        "\n",
        "def grad_sigmoid(t):\n",
        "    \"\"\"return the derivative of sigmoid on t.\"\"\"\n",
        "    return sigmoid(t) * (1 - sigmoid(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will initialize our data and parameters with some random numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array([0.01, 0.02, 0.03, 0.04])\n",
        "W = {\n",
        "    \"w_1\": np.ones((4, 5)),\n",
        "    \"w_2\": np.ones(5)\n",
        "}\n",
        "y = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, let's implement the forward pass. If you implement it correctly, you should see that your code can pass the test successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_feed_forward(x, W):\n",
        "    \"\"\"Do feed forward propagation.\"\"\" \n",
        "    x_0 = x\n",
        "    z_1 = W[\"w_1\"].T @ x_0\n",
        "    x_1 = sigmoid(z_1)\n",
        "    z_2 = W[\"w_2\"].T @ x_1\n",
        "    y_hat = sigmoid(z_2)\n",
        "    \n",
        "    return z_1, z_2, y_hat\n",
        "\n",
        "try:\n",
        "    expected = 0.93244675427215695\n",
        "    _, _, yours = simple_feed_forward(x, W)\n",
        "    assert np.sum((yours - expected) ** 2) < 1e-15\n",
        "    print(\"Your implementation is correct!\")\n",
        "except:\n",
        "    print(\"Your implementation is not correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Backward pass\n",
        "\n",
        "We now have a working implementation of our network! However, if we want to be able to train it using gradient descent, we need to be able to compute its gradient. Let's do that.\n",
        "\n",
        "We will use the squared error as our loss function, i.e.,\n",
        "$$\\ell(y,\\hat y)=\\frac{1}{2}(\\hat y-y)^2$$\n",
        "\n",
        "\n",
        "### Exercise\n",
        "Evaluate the derivative of $\\mathcal{L}(\\bf w)=\\ell(y, f_{\\bf w}(\\bf x))$ with respect to $w_{i,1}^{(2)}$ and $w_{i,j}^{(1)}$ for a single training sample $(\\bf x, y)$, by following the backpropagation algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise [EVALUATED 2]\n",
        "Now that we have derived the backward pass analytically, let's implement it in Python!\n",
        "\n",
        "*Hint*: You might want to slightly change `simple_feed_forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_backpropagation(y, x, W):\n",
        "    \"\"\"Do backpropagation and get delta_W.\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "  \n",
        "try:\n",
        "    expected = {\n",
        "        'w_1': np.array([\n",
        "            [ -1.06113639e-05,  -1.06113639e-05,  -1.06113639e-05, -1.06113639e-05,  -1.06113639e-05],\n",
        "            [ -2.12227277e-05,  -2.12227277e-05,  -2.12227277e-05, -2.12227277e-05,  -2.12227277e-05],\n",
        "            [ -3.18340916e-05,  -3.18340916e-05,  -3.18340916e-05, -3.18340916e-05,  -3.18340916e-05],\n",
        "            [ -4.24454555e-05,  -4.24454555e-05,  -4.24454555e-05, -4.24454555e-05,  -4.24454555e-05]]),\n",
        "        'w_2': np.array(\n",
        "            [-0.00223387, -0.00223387, -0.00223387, -0.00223387, -0.00223387])\n",
        "    }\n",
        "    yours = simple_backpropagation(y, x, W)\n",
        "    assert np.sum(\n",
        "        [np.sum((yours[key] - expected[key]) ** 2) for key in expected.keys()]) < 1e-15\n",
        "    print(\"Your implementation is correct!\")\n",
        "except:\n",
        "    print(\"Your implementation is not correct!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3) Bonus: Effect of regularization\n",
        "\n",
        "One of the first things we learn about neural networks is that their loss landscape is not convex. This means that analyzing how different design choices will affect their performance precisely is generally very hard. Fortunately, however, many times we can get an intuition of the behaviour of a neural network by taking a few approximations. We will now explore one of those. In particular, we will use some simple approximations to explore what is the effect of regularization on the weights of a neural network. \n",
        "\n",
        "Let $\\bf w$ be the weight vetor of all weights in the neural network, and recall that we do not normally penalize the bias term, so let's ignore it for the rest of our derivations. Furthermore, let $\\bf w^\\star$ denote a parameter that minimizes the cost function $\\mathcal L$ for the given test set (where the cost functions does not include the regularization). We would like to study how the optimal weight changes if we include some regularization.\n",
        "\n",
        "In order to make the problem tractable, assume that $\\mathcal L(\\bf w)$ can be locally expanded around the optimal parrameter $\\bf w^\\star$ in the form\n",
        "$$\\mathcal L(\\bf w) =\\mathcal L(\\bf w^\\star)+\\frac{1}{2}(\\bf w-\\bf w^\\star)^\\top\\bf H(\\bf w-\\bf w^\\star),$$\n",
        "where $\\bf H$ denotes the Hessian, whose components are the entries\n",
        "$$\\cfrac{\\partial^2 \\mathcal{L}}{\\partial \\bf w_i \\partial \\bf w_j }$$\n",
        "\n",
        "Now, let's add a regularization term of the form $\\frac{1}{2}\\mu\\|\\bf w\\|^2_2$.\n",
        "\n",
        "## Exercise\n",
        "1. Show that the optimum weight vector for the regularized problem is given by $$\\bf Q(\\bf \\Lambda+\\mu\\bf I)^{-1}\\bf \\Lambda\\bf Q^\\top \\bf w^\\star$$ where $\\bf H=\\bf Q\\bf\\Lambda\\bf Q^\\top$ represents the eigenvalue decomposition of the symmetric matrix $\\bf H$, i.e., $\\bf Q$ is an orthonormal matrix, and $\\bf \\Lambda$ is a diagonal matrix whose entries are non-negative and decreasing along the diagonal.\n",
        "2. Show that $(\\bf\\Lambda+\\mu\\bf I)^{-1}\\bf\\Lambda$ is again a diagonal matrix whose $i$-th entry is now $\\lambda_i/(\\lambda_i+\\mu)$.\n",
        "3. Argue that along the dimensions of the eigenvectors of $\\bf H$ that correspond to large eigenvalues, essentially no changes occur in the weights, but that along the dimensions of eigenvectors of very small eigenvalues the weight is drastically decreased. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "FoIL_TP7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
