{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GteYUTHvkZtc"
   },
   "source": [
    "# EE-411 Fundamentals of inference and learning, EPFL \n",
    "## Exercise Session 5: Regularized least-squares and gradient descent methods\n",
    "\n",
    "In this fifth set of exercises,  we will discuss  how to solve least-squares problems with various techniques, and in particular the workhorse of modern machine learning: gradient descent. We shall first see how regularized least-square methods such as Ridge and LASSO perform on a simple statistical problem. Then, we will introduce different gradient descent methods and we use them on some simple surfaces, studying their performances. From the start, let's emphasize that doing gradient descent on a surface is different from performing gradient descent on a loss function in Machine Learning. The reason is that in ML not only do we want to find good minima, we want to find good minima that generalize well to new data. Despite this crucial difference, we can still build intuition about gradient descent methods by applying them to simple surfaces (see related blog posts [here](http://ruder.io/optimizing-gradient-descent/)).\n",
    "\n",
    "**What you will learn today:** In this fifth notebook, you will have your first hands-on application on parametric methods in machine learning. We shall see how to use python to solve least-squared problems, and how to implement gradient descent for optimizing different functions. You will also gain intuition about various gradient descent methods by visualizing and applying these methods to some simple two-dimensional surfaces. Methods studied include ordinary gradient descent, gradient descent with momentum and NAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8JmlYbeMOd9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 9)\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S2tgbBlO-jf"
   },
   "source": [
    "Classification isn't the only task within the scope of supervised machine learning. **Regression** is another equally important task with many complementary features. In regression, we are asked to make a real-valued prediction based upon a dataset of samples. As in classification, we can think of our dataset as an $N\\times P$ matrix, with $N$ samples and $P$ features, but now instead of binary *labels* corresponding to a partition of the dataset, we are interested in matching real-valued *response* variables. \n",
    "\n",
    "In the case of simple linear regression, the problems we look at are of the form\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\mathbf{\\xi},$$\n",
    "where $\\mathbf{y}$ are our response variables, the matrix $\\mathbf{X}$ is our dataset and the vector $\\mathbf{\\xi}$ represents some noise or uncertainty on our model. Our hope is to find the *explanatory* variables $\\mathbf{w}$ which are able to match our observations and which generalize to future observations.\n",
    "\n",
    "Here, we will consider solving regressions via **maximum a posteriori (MAP)** approaches. From the Bayesian perspective, we are often attempting to solve the problem\n",
    "$${\\rm arg~min}_{\\mathbf{w}} \\; \\big(-\\log P(\\mathbf{y}|X, \\mathbf{w}) - \\log P(\\mathbf{w})\\big).\n",
    "$$ \n",
    "\n",
    "Now, if we return to our definition of linear regression, we assume that the additive noise is *iid* Gaussian and also a uniform prior on the value of $\\mathbf{w}$, we arrive (as seen during the lecture) at the ordinary least squares (OLS) problem, namely\n",
    "\n",
    "$$\n",
    "{\\rm arg~min}_{\\mathbf{w}} \\quad ||\\mathbf{y} - X\\mathbf{w} ||_2^2,\n",
    "$$\n",
    "\n",
    "which we can interpret as a **maximum likelihood (ML)** solution.\n",
    "\n",
    "Regressions, and specifically linear regressions, often form the basis of the ___\"Do The Simplest Thing That Could Possibly Work.\"___ Many times in production, simple regressions provide good-enough performance which maximizes the trade-off against engineering costs (coding time, computer time). And, as we will later see, regressions can even be applied to solve classification tasks, either directly or using instead *logistic* regression. So, truly, regressions should be one of the first step when encountering a new data-science or ML problem. Things can only, hopefully, go up from here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7fiQJ6McybY"
   },
   "source": [
    "# A) Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7FkdVg5PRgs"
   },
   "source": [
    "## Ridge regression\n",
    "First, we need to discuss the many ways to solve the minimization problem. We are going to assume that we are interested to the Ridge problem:\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = {\\rm arg~min}_{\\mathbf{w}} \\left( \\frac 1n ||\\mathbf{y} - X \\mathbf{w} ||_2^2 + \\lambda \\|  \\mathbf{w}\\|_2^2 \\right)$$ \n",
    "with $\\mathbf{y} \\in \\mathbb R^n$,$\\mathbf{w} \\in \\mathbb R^p$, and $X$ a $n \\times p$ matrix. We have shown in the lecture that the solution should satify the normal equations:\n",
    "$$\n",
    "(X^T X + n \\lambda {\\mathbb 1}_p) \\hat {\\mathbf w} = X^T {\\mathbf y}\n",
    "$$\n",
    "For any positive $\\lambda$ the solution is unique (*question: why?*) and can be written either as (*question: why are these matrices invertible?*) \n",
    "$$\n",
    "\\hat {\\mathbf w} = (X^T X + n\\lambda {\\mathbb 1}_p)^{-1} X^T {\\mathbf y} = X^T (X X^T + n \\lambda {\\mathbb 1}_n)^{-1}  {\\mathbf y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYB2Kmyq-xPK"
   },
   "source": [
    "We shall see how to use these formulae. First we import the function `inv`, that allows us to invert matrices (indeed, if they are invertible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvjZO1OcU8gq"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aCccwwh-saR"
   },
   "source": [
    "Then we generate an artificial set of data for the problem, using `randn` to create an $n\\times p$ matrix $X$ and a $n$-dimensional vector $y$, in both of which every element is generated randomly according to a Normal distribution of unitary variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snP9oHO18Dfu"
   },
   "outputs": [],
   "source": [
    "p = 50 #number of features\n",
    "n = 250 #number of data points\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "y = np.random.randn(n,1)\n",
    "lamb = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOQ3QpfV-vKe"
   },
   "source": [
    "Compute the solution using the two methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nrep = 100\n",
    "\n",
    "w1 = inv(X.T@X+n*lamb*np.eye(p,p))@X.T@y\n",
    "t1 = timeit.timeit('inv(X.T@X+n*lamb*np.eye(p,p))@X.T@y',\"from __main__ import n,p,X,y,lamb,inv,np\", number=Nrep)/Nrep\n",
    "print(\"formula 1 took :\",t1,\"ms \\n and the result is w1=\",w1[:3],\"...\")\n",
    "\n",
    "w2 = X.T@inv(X@X.T+n*lamb*np.eye(n,n))@y\n",
    "t2 = timeit.timeit('X.T@inv(X@X.T+n*lamb*np.eye(n,n))@y',\"from __main__ import n,p,X,y,lamb,inv,np\", number=Nrep)/Nrep\n",
    "print(\"formula 2 took :\",t2,\"ms \\n and the result is w2=\",w2[:3],\"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9NMKaYyYVpLN"
   },
   "source": [
    "We repeat your experiment for different values of $n$ and $p$ to understand when it is better to use one or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kev08W-BBcSv"
   },
   "outputs": [],
   "source": [
    "# It takes a lot to run!\n",
    "Nt=15\n",
    "table_p = np.linspace(50,250,num=Nt).astype(int)\n",
    "table_n = np.linspace(50,250,num=Nt).astype(int)\n",
    "time1 = np.zeros((Nt,Nt))\n",
    "time2 = np.zeros((Nt,Nt))\n",
    "lamb = 0.01\n",
    "Nrep = 100\n",
    "\n",
    "for i,n in enumerate(table_n):\n",
    "  for j,p in enumerate(table_p):\n",
    "    X = np.random.randn(n,p)\n",
    "    y = np.random.randn(n,1)\n",
    "\n",
    "    time1[i][j] = timeit.timeit('inv(X.T@X+n*lamb*np.eye(p,p))@X.T@y',\"from __main__ import n,p,X,y,lamb,inv,np\", number=Nrep)/Nrep\n",
    "    time2[i][j] = timeit.timeit('X.T@inv(X@X.T+n*lamb*np.eye(n,n))@y',\"from __main__ import n,p,X,y,lamb,inv,np\", number=Nrep)/Nrep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1632746420304,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -120
    },
    "id": "DBEV97SJIWER",
    "outputId": "f4f14b7d-44b6-425a-e849-7da08f699cf3"
   },
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(table_n, table_p)\n",
    "Z = (time1-time2).reshape(X.shape)\n",
    "\n",
    "im = plt.imshow(Z, cmap=plt.cm.RdBu, extent=(0, 400,0, 400), origin='lower')  \n",
    "cset = plt.contour(Z, [0.], linewidths=2,\n",
    "                   cmap=plt.cm.Set2,\n",
    "                  extent=(0, 400, 0, 400))\n",
    "plt.clabel(cset, inline=True, fmt='%1.3f', fontsize=20)\n",
    "plt.colorbar(im)  \n",
    "plt.xlabel('n')\n",
    "plt.ylabel('p')\n",
    "plt.xlim(0,400)\n",
    "plt.title('time1 - time2 (ms)')\n",
    "plt.ylim(0,400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZyGmDncYR2w"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D9Dzi9TWBxc"
   },
   "source": [
    "An alternative to solve the problem is to use gradient descent! In order to keep number from exploding, we first define the cost function divided by the number of data as\n",
    "$$\n",
    "{\\cal L}({\\mathbf{w}}) = \\frac {1}{2}  \\bigg( \\frac 1n ||\\mathbf{y} - X \\mathbf{w} ||_2^2 +  \\lambda \\|  \\mathbf{w}\\|_2^2 \\bigg)$$\n",
    "\n",
    "\n",
    "Therefore, the gradient reads\n",
    "$$\n",
    "\\nabla_\\mathbf{w} {\\cal L}(\\mathbf{w}) = -\\frac 1n (X^T y - X^TX \\mathbf {w}) + \\lambda \\mathbf {w} = \\frac 1n (X^T (X {\\mathbf w} - y)) + \\lambda \\mathbf {w}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX-6VdcoYcXL"
   },
   "source": [
    "We import the function `norm`, which allows us to compute different matrix norms, and we choose some random initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9d7lQlKXtsw"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "w0 = np.random.rand(p,1) #Pick some random weights to start the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0TuwAMkvYn3r"
   },
   "source": [
    "#### 1) Calling $\\eta$ the step size and $t_{max}$ the number of iterations, design the function `gradient_descent` which, given $X$, $y$ and the parameters $\\mathbf{w_0}$, $t_{max}$, $\\lambda$ and $\\eta$, returns the GD estimation for $w$ and the behaviour of the cost function ${\\cal L}(\\mathbf{w})$ with the different time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w0, iterations, l2, eta):\n",
    "    w = w0\n",
    "    past_costs = []\n",
    "    for _ in range(iterations):\n",
    "        prediction =  X@w\n",
    "        error = prediction - y\n",
    "        cost = (1/(2*n) * np.dot(error.T, error) + 1/2.0 * l2 * np.dot(w.T,w))[0,0]\n",
    "        past_costs.append(cost)\n",
    "        w = w - (eta * (np.dot(X.T, error)/n+l2*w))\n",
    "    return w, past_costs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MgiHrbmIb-kF"
   },
   "source": [
    "Fixing $n=30$, $p=3$, $t_{max}=50$, $\\lambda=0.001$ for each value of $\\eta$ in $[0.03,0.1,0.3,1,2,3]$ :\n",
    "* Use the function you designed to plot the behaviour of the cost function vs the number of iterations\n",
    "* Compute the distance between the final estimation of $\\mathbf{w}$ and the exact one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30\n",
    "p=3\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "y = np.random.randn(n,1)\n",
    "w0 = np.random.rand(p,1)\n",
    "lamb = 0.001\n",
    "\n",
    "max = 50 #No. of iterations\n",
    "eta = [0.03,0.1,0.3,1,2,3]\n",
    "\n",
    "w_ref = inv(X.T@X+n*lamb*np.eye(p,p))@X.T@y\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(23,15), sharex=True)\n",
    "plt.suptitle('Cost Function')\n",
    "for i,e in enumerate(eta):\n",
    "  w, past_costs = gradient_descent(X, y, w0, max, lamb, e)\n",
    "  axs[int(i/3)][i%3].plot(past_costs)\n",
    "  axs[int(i/3)][i%3].set_title(f'$\\eta = {e}$, $\\Delta w = {norm(w_ref-w):1.1e}$')\n",
    "  axs[int(i/3)][i%3].set_xlabel('No. of iterations')\n",
    "  axs[int(i/3)][i%3].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = np.arange(0.01,2,0.01)\n",
    "table_w = np.zeros(len(eta))\n",
    "\n",
    "for j,e in enumerate(eta):\n",
    "  w, past_costs = gradient_descent(X, y, w0, max, lamb, e)\n",
    "  table_w[j] = norm(w_ref-w)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.title('Distance from the exact solution after 50 time steps')\n",
    "plt.xlabel('$\\eta$')\n",
    "plt.plot(eta,table_w)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwXogYngdHmO"
   },
   "source": [
    "## Ordinary Least Square and implicit regularization\n",
    "\n",
    "Now, let's focus on the case where $n<p$ and let's compare the solution found by gradient descent when $\\lambda=0$, with the one we find using just ordinary least squares:\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = {\\rm arg~min}_{\\mathbf{w}} \\quad ||\\mathbf{y} - X \\mathbf{w} ||_2^2 \n",
    "$$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "K8jBZ0W1dShP"
   },
   "source": [
    "We try to use the formula\n",
    "\n",
    "$$\n",
    "\\hat {\\mathbf w} = (X^T X )^{-1} X^T {\\mathbf y}\n",
    "$$\n",
    "\n",
    "to get the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(X.T@X)@(X.T@y) - w_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgOMOaR_dVHf"
   },
   "source": [
    "###\n",
    " \n",
    "Full rank, coincides with the regularized solution\n",
    "\n",
    "###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "88_cTK56dziQ"
   },
   "source": [
    "Then, we can use the least norm solution:\n",
    "$$\n",
    "\\hat {\\mathbf w}_{ln} = X^T (X X^T)^{-1}  {\\mathbf y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T@inv(X@X.T)@y - w_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1N0Hqmad_92"
   },
   "source": [
    "###\n",
    "\n",
    "Not full rank, the solution found by the (pseudo)inverse makes no sense\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(X@X.T)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ea6XummOeHgy"
   },
   "source": [
    "#### 2) Fix $p=100$ and $n=20$ and, taking $\\lambda=0$, compare the solution obtained using $\\hat{\\mathbf w}_{ln}$ with \n",
    "* the one obtained by gradient descent with a random initilization\n",
    "* the one obtained by gradient descent with a zero initilization\n",
    "\n",
    "#### How can you explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100 #data dimension\n",
    "n = 20 #number of datapoints\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "y = np.random.randn(n,1)\n",
    "lamb = 0\n",
    "eta = 0.1 #Step size\n",
    "max = 100 #No. of iterations\n",
    "w0_r = np.random.randn(p,1) #Pick some random values to start with\n",
    "w0_z = np.zeros((p,1)) #Start with a zero vector\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "w_r, past_costs_r = gradient_descent(X, y, w0_r, max, lamb, eta)\n",
    "w_z, past_costs_z = gradient_descent(X, y, w0_z, max, lamb, eta)\n",
    "plt.title('Cost Function')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(past_costs_r, label='Random initialization')\n",
    "plt.plot(past_costs_z, label='Null initialization')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "w_ref = X.T@inv(X@X.T)@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'Difference of w_r from optimum w is {:.3f}'.format(norm(w_ref-w_r)))\n",
    "print(r'Difference of w_z from optimum w is {}'.format(norm(w_ref-w_z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCPMiabte909"
   },
   "source": [
    "## LASSO and sub-gradient descent\n",
    "\n",
    "Another cost function that shall be useful is the one corresponding to LASSO:\n",
    "$$\n",
    "{\\cal L}({\\mathbf{w}}) =  \\frac{1}{2n} ||\\mathbf{y} - X \\mathbf{w} ||_2^2 +  \\lambda \\|  \\mathbf{w}\\|_1 \n",
    "$$\n",
    "\n",
    "One can define the subgradient as\n",
    "$$\n",
    "\\nabla_\\mathbf{w} {\\cal L}(\\mathbf{w}) = -\\frac 1n (X^T y - X^TX \\mathbf {w}) + \\lambda {\\rm sign} ({\\mathbf w}) = \\frac 1n (X^T (X {\\mathbf w}-y)) + \\lambda {\\rm sign}({\\mathbf w})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0n1LStxrhv7A"
   },
   "source": [
    "#### 3) Using  `np.sign()` to implement the sign function, construct the function `gradient_descent_lasso` that returns an estimation for $w$ and the behaviour of the cost function ${\\cal L}(\\mathbf{w})$ with the different time steps. Having done that, use this function to plot the cost function for a new dataset with  $p=100$ and $n=20$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_lasso(X, y, w, iterations, l1, eta):\n",
    "    past_costs = []\n",
    "    n= len(y)\n",
    "    for i in range(iterations):\n",
    "        prediction =  X@w\n",
    "        error = prediction - y\n",
    "        cost = (1/(2*n) * np.dot(error.T, error) + l1 * np.linalg.norm((w), ord=1))[0,0]\n",
    "        past_costs.append(cost)\n",
    "        w = w - eta *( (1/n) * (np.dot(X.T, error)+l1*np.sign(w)))\n",
    "    return w, past_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100 #data dimension\n",
    "n = 20 #number of datapoints\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "y = np.random.randn(n,1)\n",
    "lamb = 0.1\n",
    "eta = 0.1 #Step size\n",
    "max = 500 #No. of iterations\n",
    "w0 = np.random.randn(p,1) #Pick some random values to start with\n",
    "w0 = np.zeros((p,1)) #Start with a zero vector\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "w, past_costs = gradient_descent_lasso(X, y, w0, max, lamb, eta)\n",
    "plt.title('Cost Function')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(past_costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho0Uml9ciVj_"
   },
   "source": [
    "## LASSO with Iterartive soft Thresholding [Evaluated]\n",
    "\n",
    "#### 3b) Find and implement Iterative soft Thresholding algorithm to solve the LASSO problem. Compare the results with the ones obtained by the sub-gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAAgaQjRf3RN"
   },
   "source": [
    "# B) Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKtzSG28Jkxu"
   },
   "source": [
    "We will consider two simple surfaces: A quadratic minimum of the form \n",
    "\n",
    "$$\n",
    "z=ax^2+by^2\\,,\n",
    "$$\n",
    "\n",
    "and the [Matyas function](https://en.wikipedia.org/wiki/Test_functions_for_optimization), a convex function often used to test optimization problems of the form:\n",
    "\n",
    "$$\n",
    "z(x,y) = 0.26(x^2 + y^2) - 0.48xy\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBDQptXlJ9S1"
   },
   "source": [
    "These surfaces can be plotted using the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jPNTMi2f3RP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make 3D plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "#Function for plotting \n",
    "\n",
    "def plot_surface(x, y, z, azim=-60, elev=40, dist=10, cmap=\"RdYlBu_r\"):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot_args = {'rstride': 1, 'cstride': 1, 'cmap':cmap, 'linewidth': 20, 'antialiased': True, 'vmin': -2, 'vmax': 2} #parameters for the plots\n",
    "    ax.plot_surface(x, y, z, **plot_args)\n",
    "    ax.view_init(azim=azim, elev=elev) #point of view\n",
    "    ax.dist=dist\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_zlim(-2, 2)\n",
    "    \n",
    "    plt.xticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n",
    "    plt.yticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n",
    "    ax.set_zticks([-2, -1, 0, 1, 2])\n",
    "    ax.set_zticklabels([\"-2\", \"-1\", \"0\", \"1\", \"2\"])\n",
    "    \n",
    "    ax.set_xlabel(\"x\", fontsize=18)\n",
    "    ax.set_ylabel(\"y\", fontsize=18)\n",
    "    ax.set_zlabel(\"z\", fontsize=18)\n",
    "    return fig, ax;\n",
    "\n",
    "#Overlay the trajectory of GD on a contour plot\n",
    "\n",
    "def overlay_trajectory_contour(ax,trajectory, label,color='k',lw=2):\n",
    "    xs=trajectory[:,0]\n",
    "    ys=trajectory[:,1]\n",
    "    ax.plot(xs,ys, color, label=label,lw=lw)\n",
    "    return ax;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FokDMNyqKHi-"
   },
   "source": [
    "We now define the functions for the Matyas function and the respective gradient, that we will need for the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUJW68raf3RR"
   },
   "outputs": [],
   "source": [
    "# Define matyas_surface and the respective gradient\n",
    "\n",
    "def matyas_surface(x,y):\n",
    "    return 0.26*(x**2 + y**2) - 0.48*x*y\n",
    "\n",
    "def grad_matyas_surface(params):\n",
    "    x=params[0]\n",
    "    y=params[1]\n",
    "    grad_x= 0.52*x-0.48*y\n",
    "    grad_y= 0.52*y-0.48*x\n",
    "    return [grad_x,grad_y]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GKYArSAdKcBn"
   },
   "source": [
    "#### 4) Now do the  same for the quadratic minimum surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minima_surface and the respective gradient\n",
    "\n",
    "def minima_surface(x,y,a=1,b=1):\n",
    "    return a*x**2+b*y**2-1\n",
    "\n",
    "def grad_minima_surface(params,a=1,b=1):\n",
    "    x=params[0]\n",
    "    y=params[1]\n",
    "    grad_x= 2*a*x\n",
    "    grad_y= 2*b*y\n",
    "    return [grad_x,grad_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4GMvHVjK27A"
   },
   "source": [
    "First of all, we can plot these surfaces to see how they look like (choose for example $a=1.5$ and $b=2$ for the saddle point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plots of the surfaces\n",
    "\n",
    "plt.close() # closes previous plots\n",
    "x, y = np.mgrid[-1:1:31j, -1:1:31j] #31 points between -1 and 1\n",
    "fig1,ax1=plot_surface(x,y,matyas_surface(x,y))\n",
    "plt.show()\n",
    "\n",
    "#your code (add the other one)\n",
    "\n",
    "fig2,ax2=plot_surface(x,y,minima_surface(x,y,1.5,2),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FRmxx5Bf3RU"
   },
   "source": [
    "## Gradient descent with and without momentum\n",
    "\n",
    "Now, let's visualize various gradient descent algorithms used in machine learning. We will be especially interested in trying to understand how various hyperparameters, especially the learning rate, affect our performance.\n",
    "\n",
    "As always, we will work with some parameters $\\mathbf{w}$ and we have an energy function we are trying to minimize, usually denoted by ${\\cal L}(\\mathbf{w})$.\n",
    "\n",
    "<b>Gradient Descent</b>\n",
    "\n",
    "We start by considering a simple gradient descent method. In this method,\n",
    "we will take steps in the direction of the local gradient. Given some parameters $\\mathbf{w}$, we adjust them at each iteration so that\n",
    "\n",
    "$$\\mathbf{w}_{t+1}= \\mathbf{w}_t - \\eta_t \\nabla_\\mathbf{w} {\\cal L}(\\mathbf{w}),$$\n",
    "\n",
    "where we have introduced the learning rate $\\eta_t$ that controls how large a step we take. In general, the algorithm is extremely sensitive to the choice of $\\eta_t$. If $\\eta_t$ is too large, then one can wildly oscillate around minima and miss important structure at small scales. This problem is amplified if our gradient computations are noisy and inexact (as is often the case in machine learning applications). If $\\eta_t$ is too small, then the learning/minimization procedure becomes extremely slow. This raises the natural question: <i> What sets the natural scale for the learning rate and how can we adaptively choose it?</i>\n",
    "\n",
    "<b>Gradient Descent with Momentum</b>\n",
    "\n",
    "One problem with gradient descent is that it has no memory of where the \"ball rolling down the hill\" comes from. This can be an issue when there are many shallow minima in our landscape. If we make an analogy with a ball rolling down a hill, the lack of memory is equivalent to having no inertia or momentum (i.e. completely overdamped dynamics). Without momentum, the ball has no kinetic energy and cannot climb out of shallow minima. \n",
    "\n",
    "Momentum becomes especially important when we start thinking about **stochastic gradient descent** with noisy, stochastic estimates of the gradient. In this case, we should remember where we were coming from and not react drastically to each new update.\n",
    "\n",
    "\n",
    "\n",
    "Inspired by this, we can add a memory or momentum term to the stochastic gradient descent term above:\n",
    "\n",
    "$$\n",
    "v_{t}=\\gamma v_{t-1}+\\eta_{t}\\nabla_\\mathbf{w} {\\cal L}(\\mathbf{w}_t),\\\\\n",
    "\\mathbf{w}_{t+1}= \\mathbf{w}_t -v_{t},\n",
    "$$\n",
    "\n",
    "with $0\\le \\gamma < 1$ called the momentum parameter. When $\\gamma=0$, this reduces to ordinary gradient descent, and increasing $\\gamma$ increases the inertial contribution to the gradient. From the equations above, we can see that typical memory lifetimes of the gradient is given by $(1-\\gamma)^{-1}$. For $\\gamma=0$ as in gradient descent, the lifetime is just one step. For $\\gamma=0.9$, we typically remember a gradient for ten steps. We will call this gradient descent with **classical momentum** or CM for short.\n",
    "\n",
    "**NAG** \n",
    "\n",
    "A final widely used variant of gradient descent with momentum is called the Nesterov accelerated gradient (NAG). In NAG, rather than calculating the gradient at the current position, one calculates the gradient at the position momentum will carry us to at time $t+1$, namely, $\\mathbf{w}_t -\\gamma v_{t-1}$. Thus, the update becomes\n",
    "$$\n",
    "v_{t}=\\gamma v_{t-1}+\\eta_{t}\\nabla_\\mathbf{w} {\\cal L}(\\mathbf{w}_t-\\gamma v_{t-1})\\\\\n",
    "\\mathbf{w}_{t+1}= \\mathbf{w}_t -v_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nZFI-RULaAF"
   },
   "source": [
    "Let's now write the functions to implement these different versions of gradient descent, starting with standard GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DIcDwQAf3RU"
   },
   "outputs": [],
   "source": [
    "#Simple gradient descent\n",
    "\n",
    "def gd(grad, init, n_epochs=1000, eta=10**-4, noise_strength=0):\n",
    "    params=np.array(init)\n",
    "    param_traj=np.zeros([n_epochs+1,2])\n",
    "    param_traj[0,]=init\n",
    "    v=0;\n",
    "    for j in range(n_epochs):\n",
    "        noise=noise_strength*np.random.randn(params.size)\n",
    "        v=eta*(np.array(grad(params))+noise)\n",
    "        params=params-v\n",
    "        param_traj[j+1,]=params\n",
    "    return param_traj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fDFFz5nMLm4H"
   },
   "source": [
    "#### 5) Now write yourself the functions implementing GD with CM and NAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent with momentum\n",
    "\n",
    "def gd_with_mom(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9,noise_strength=0):\n",
    "    params=np.array(init)\n",
    "    param_traj=np.zeros([n_epochs+1,2])\n",
    "    param_traj[0,]=init\n",
    "    v=0\n",
    "    for j in range(n_epochs):\n",
    "        noise=noise_strength*np.random.randn(params.size)\n",
    "        v=gamma*v+eta*(np.array(grad(params))+noise)\n",
    "        params=params-v\n",
    "        param_traj[j+1,]=params\n",
    "    return param_traj\n",
    "\n",
    "#Nesterov accelerated gradient descent\n",
    "\n",
    "def NAG(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9,noise_strength=0):\n",
    "    params=np.array(init)\n",
    "    param_traj=np.zeros([n_epochs+1,2])\n",
    "    param_traj[0,]=init\n",
    "    v=0\n",
    "    for j in range(n_epochs):\n",
    "        noise=noise_strength*np.random.randn(params.size)\n",
    "        params_nesterov=params-gamma*v\n",
    "        v=gamma*v+eta*(np.array(grad(params_nesterov))+noise)\n",
    "        params=params-v\n",
    "        param_traj[j+1,]=params\n",
    "    return param_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ul7Wkj-f3RV"
   },
   "source": [
    "## Experiments with GD, CM, and NAG\n",
    "\n",
    "Let us play with these methods to gain some intuition.\n",
    "\n",
    "Let's look at the dependence of GD on learning rate in the Matyas surface. We do the plot below for $\\eta=0.1, 0.5, 1, 2$.\n",
    "- What are the qualitatively different behaviors that arise as $\\eta$ is increased?\n",
    "- What does this tell us about the importance of choosing learning parameters? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1633883644352,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -120
    },
    "id": "R3dkR3gNMTI8",
    "outputId": "754124dc-4afc-421b-e9be-d658a8f9a13c"
   },
   "outputs": [],
   "source": [
    "# Investigate effect of learning rate in GD\n",
    "plt.close()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n",
    "z=np.abs(matyas_surface(x,y))\n",
    "\n",
    "#initial point\n",
    "init4=[0,4]\n",
    "init2=[-4,2]\n",
    "init3=[-3,-2]\n",
    "init1=[3,0.4]\n",
    "eta1=0.1\n",
    "eta2=0.5\n",
    "eta3=1\n",
    "eta4=2\n",
    "\n",
    "ax1.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "ax1.set_xlim((-4.5, 4.5))\n",
    "ax1.set_ylim((-4.5, 4.5))\n",
    "\n",
    "gd_1=gd(grad_matyas_surface,init1, n_epochs=100, eta=eta1)\n",
    "gd_2=gd(grad_matyas_surface,init2, n_epochs=100, eta=eta2)\n",
    "gd_3=gd(grad_matyas_surface,init3, n_epochs=100, eta=eta3)\n",
    "gd_4=gd(grad_matyas_surface,init4, n_epochs=100, eta=eta4)\n",
    "\n",
    "overlay_trajectory_contour(ax1,gd_1,'$\\eta=$%s'% eta1,'g--*', lw=0.5)\n",
    "overlay_trajectory_contour(ax1,gd_2,'$\\eta=$%s'% eta2,'b->', lw=0.5)\n",
    "overlay_trajectory_contour(ax1,gd_3,'$\\eta=$%s'% eta3,'->', lw=0.5)\n",
    "overlay_trajectory_contour(ax1,gd_4,'$\\eta=$%s'% eta4,'c-o', lw=0.5)\n",
    "\n",
    "ax1.plot(0,0, 'r*', markersize=18)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AxtiqlhOMrJU"
   },
   "source": [
    "#### 6) Do the same for the quadratic minimum. How do these change if we change $a$ and $b$ above? In particular how does anisotropy change the learning behavior? [Bonus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "URsUsf6bSikG"
   },
   "source": [
    "#### 7) Make similar plots for CM and NAG. How do the learning rates for these procedures compare with those for GD? [Bonus]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FoIL_ex4_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
