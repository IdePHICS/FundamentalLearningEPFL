{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will approach for the first time PyTorch. \n",
    "First we see how `torch` can help us to avoid long `numpy` code and then we will see how to use `torch` to build a simple neural network for classification.\n",
    "\n",
    "# Part 1: From Numpy to PyTorch\n",
    "\n",
    "**What will you learn in this part**:\n",
    "- Tensors syntax\n",
    "- Autograd\n",
    "- Neural Network modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to last week exercise and migrate it to PyTorch. Luckily, the syntax is almost identical. The main difference is that *arrays* are replaced by *tensors*, and all the `np.*` functions become `torch.*`. For more advanced functionalities, we refer you to the [official documentation][torch_doc].\n",
    "\n",
    "[torch_doc]: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "## Single layer MLP in Numpy\n",
    "\n",
    "Recall the feedforward neural network with a single hidden layer.\n",
    "\n",
    "![simple_mlp](./simple_mlp.png)\n",
    "\n",
    "Below is the Numpy implementation of the activations and the feedforward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "def np_sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def np_grad_sigmoid(t):\n",
    "    \"\"\"return the derivative of sigmoid on t.\"\"\"\n",
    "    return np_sigmoid(t) * (1 - np_sigmoid(t))\n",
    "\n",
    "def np_mlp(\n",
    "    x: NDArray[np.float64], w_1: NDArray[np.float64], w_2: NDArray[np.float64]\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"Feed forward propagation on MLP\n",
    "\n",
    "    Args:\n",
    "        x (NDArray[np.float_]): Input vector of shape (d_in,)\n",
    "        w_1 (NDArray[np.float_]): Parameter matrix of first hidden layer, of shape (d_in, d_hid)\n",
    "        w_2 (NDArray[np.float_]): Parameter vector of output layer, of shape (d_hid,)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[NDArray[np.float], NDArray[np.float], NDArray[np.float]]: Three\n",
    "            arrays `y_hat`, `z_1`, `z_2`, containing repsectively the output and\n",
    "            the two preactivations.\n",
    "    \"\"\"\n",
    "    z_1 = w_1.T @ x\n",
    "    x_1 = np_sigmoid(z_1)\n",
    "    z_2 = w_2.T @ x_1\n",
    "    y_hat = np_sigmoid(z_2)\n",
    "\n",
    "    return y_hat, z_1, z_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the backpropagation with the Mean-squared error loss $\\mathcal L (y, \\hat y) = \\frac{1}{2} \\left( y - \\hat y \\right)^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mlp_backpropagation(\n",
    "    y: NDArray[np.int_],\n",
    "    x: NDArray[np.float64],\n",
    "    w_2: NDArray[np.float64],\n",
    "    y_hat: NDArray[np.float64],\n",
    "    z_1: NDArray[np.float64],\n",
    "    z_2: NDArray[np.float64],\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"Do backpropagation and get parameter gradients.\n",
    "\n",
    "    Args:\n",
    "        y (NDArray[np.int_]): True label\n",
    "        x (NDArray[np.float64]): Input data\n",
    "        w_2 (NDArray[np.float64]): Readout layer parameters\n",
    "        y_hat (NDArray[np.float64]): MLP output\n",
    "        z_1 (NDArray[np.float64]): Hidden layer preactivations\n",
    "        z_2 (NDArray[np.float64]): Readout layer preactivations\n",
    "\n",
    "    Returns:\n",
    "        Tuple[NDArray[np.float64], NDArray[np.float64]]: Gradients of w_1 and w_2\n",
    "    \"\"\"\n",
    "    # Feed forward\n",
    "    _loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "    # Backpropogation\n",
    "    delta_2 = (y_hat - y) * np_grad_sigmoid(z_2)\n",
    "    x_1 = np_sigmoid(z_1)\n",
    "    dw_2 = delta_2 * x_1\n",
    "    delta_1 = delta_2 * w_2* np_grad_sigmoid(z_1)\n",
    "    dw_1 = np.outer(x, delta_1)\n",
    "\n",
    "    return dw_1, dw_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the MLP output and retrieve the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([0.01, 0.02, 0.03, 0.04])\n",
    "w_1_np = np.random.randn(4, 5)\n",
    "w_2_np = np.random.randn(5)\n",
    "\n",
    "y = 1\n",
    "\n",
    "y_hat_np, z_1, z_2 = np_mlp(x_np, w_1_np, w_2_np)\n",
    "dw_1_np, dw_2_np = np_mlp_backpropagation(y, x_np, w_2_np, y_hat_np, z_1, z_2)\n",
    "\n",
    "print(dw_1_np.shape)\n",
    "print(dw_2_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indeed works, but as soon as we change the neural network architecture we have to change our backpropagation function, and keep track of all the computations that involve each parameter. It is a lot of work which we want to delegate to the machine.\n",
    "This is what *automatic differentiation* does, and libraries like PyTorch implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "We can manipulate tensors as we want and, by asking for `require_grad=True`, PyTorch handles automatic differentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "a = torch.randn(10, 5)\n",
    "b = torch.ones(5, requires_grad=True)\n",
    "\n",
    "# Note that c is a scalar\n",
    "c = torch.log(a @ b).sum()\n",
    "print(\"c\", c)\n",
    "print(\"c.shape:\", c.shape)\n",
    "\n",
    "# We ask to perform backpropagation\n",
    "c.backward()\n",
    "\n",
    "print(\"b.grad:\", b.grad)\n",
    "print(\"b.grad.shape:\", b.grad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the previous code to PyTorch. Autograd is responsible of keeping track of each element in the computations, so we only need to implement the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t) -> torch.FloatTensor:\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    #vvvvv YOUR CODE HERE vvvvv#Ë™\n",
    "    return 1.0 / (1 + torch.exp(-t))\n",
    "    #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "def mlp(\n",
    "    x: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Feed forward propagation on MLP\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input vector of shape (d_in,)\n",
    "        w_1 (torch.Tensor): Parameter matrix of first hidden layer, of shape (d_in, d_hid)\n",
    "        w_2 (torch.Tensor): Parameter vector of output layer, of shape (d_hid,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Network output\n",
    "    \"\"\"\n",
    "    #vvvvv YOUR CODE HERE vvvvv#\n",
    "    z_1 = w_1.T @ x\n",
    "    x_1 = sigmoid(z_1)\n",
    "    z_2 = w_2.T @ x_1\n",
    "    y_hat = sigmoid(z_2)\n",
    "    #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can verify that the output corresponds to the numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "\n",
    "# Convert arrays to tensors. Mind that we will ask for parameters gradients!\n",
    "x = torch.tensor(x_np)\n",
    "w_1 = torch.tensor(w_1_np, requires_grad=True)\n",
    "w_2 = torch.tensor(w_2_np, requires_grad=True)\n",
    "\n",
    "y_hat = mlp(x, w_1, w_2)\n",
    "loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "#Now perform backpropagation\n",
    "loss.backward()\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "print(np.allclose(w_1.grad.numpy(), dw_1_np))\n",
    "print(np.allclose(w_2.grad.numpy(), dw_2_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "\n",
    "Computing gradients has now got much easier! :grin:\n",
    "\n",
    "Still, PyTorch provides an even easier interface to build and train neural networks, whose components are in the `torch.nn` module.\n",
    "The main tool is the `torch.nn.Module` class, from which all neural networks shall inherit. This must implement a `forward` method, and, if needed, declare its parameters in the `__init__` method. \n",
    "\n",
    "Let's convert our MLP to a proper Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_hidden: int) -> None:\n",
    "        #vvvvv YOUR CODE HERE vvvvv#\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_1 = torch.nn.Parameter(\n",
    "            torch.randn((dim_in, dim_hidden)),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.w_2 = torch.nn.Parameter(\n",
    "            torch.randn((dim_hidden, 1)),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #vvvvv YOUR CODE HERE vvvvv#\n",
    "        z_1 = self.w_1.T @ x\n",
    "        x_1 = z_1.sigmoid()  # Let's use PyTorch built-in sigmoid!\n",
    "        z_2 = self.w_2.T @ x_1\n",
    "        return z_2.sigmoid()\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, `torch.nn` comes with a lot of layers and functions which are ready to use.\n",
    "\n",
    "For instance, we have a `torch.sigmoid` function, as well as `torch.nn.Linear` layer and a `torch.nn.MSELoss` loss.\n",
    "\n",
    "Here is a minimal implementation of our forward and backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_hidden: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: Linear has a `bias` term by default!\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden, bias=False)\n",
    "        self.linear2 = nn.Linear(dim_hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).sigmoid()\n",
    "        return self.linear2(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize your model and compute the gradients with respect to the MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_IN = 5\n",
    "DIM_HIDDEN = 10\n",
    "\n",
    "x = torch.ones(DIM_IN)\n",
    "y = torch.tensor([0.1])\n",
    "\n",
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "\n",
    "my_mlp = MyMLP(DIM_IN, DIM_HIDDEN)\n",
    "print(my_mlp)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Backpropagate\n",
    "\n",
    "loss(y, my_mlp(x)).backward()\n",
    "\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "\n",
    "\n",
    "Check the sizes of the gradients of each layer and verify that they correspond to what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "\n",
    "# Note that we have multiple ways to retrieve the parameters\n",
    "#\n",
    "# This one works for the MyMLP module define above, but different modules will\n",
    "# have other submodules and parameter names\n",
    "\n",
    "gw_1 = my_mlp.linear1.get_parameter('weight')\n",
    "gw_2 = my_mlp.linear2.get_parameter('weight')\n",
    "\n",
    "print(gw_1.shape)\n",
    "print(gw_2.shape)\n",
    "\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One more thing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Sequential` module stacks the given layer one after the other.\n",
    "Still, to get more control on the forward, it is better to stick to self-defined module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_mlp = nn.Sequential(\n",
    "    nn.Linear(DIM_IN, DIM_HIDDEN, bias=False),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(DIM_HIDDEN, 1, bias=False),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "print(sequential_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hands on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you will learn in the second part**: This lab serves as an introduction to PyTorch. We will learn the different steps required in training a deep learning model with modern libraries, such as PyTorch.\n",
    "\n",
    "So, which are these steps?\n",
    "\n",
    "* Preliminaries:\n",
    "    * load the train and test datasets, `train_dataset` and `test_dataset` (MNIST in our case)\n",
    "    * turn the datasets into a \"dataloaders\": `train_dataloader` and `test_dataloader`\n",
    "    * define your `model` architecture\n",
    "    * define your `optimizer`, e.g. SGD\n",
    "\n",
    "\n",
    "* Training: Now we have all the building blocks and we need to make our model \"learn\". In most cases, the training follows a specific \"recipe\". Specifically, we feed the `model` the whole `train_dataset` using batches that come from the `train_dataloader`. We repeat this a certain number of times, called `epochs`. Each epoch consists of `batches`. So what do we do for each batch?\n",
    "    * zero out the optimizer. In essence we prepare the optimizer for the incoming data\n",
    "    * compute the output of the model $f(\\cdot)$ for our current data: $x\\mapsto f(x)$\n",
    "    * compute the loss: $\\mathcal{L}(f(x), y)$ where $y$ denotes the ground truth\n",
    "    * perform the `backpropagation` algorithm which involves computing the gradients and performing the update rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the preliminaries out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the datasets. We are going to work with MNIST and our goal is classify digits. This is a popular dataset and PyTorch offers it out-of-the-box, making our life easy! We simply need to call the corresponding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are given as PIL images. We need to convert our data to a type\n",
    "# that is readable by a Neural Network. Thus, we use the ToTensor() \"transform\"\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "BATCH_SIZE = 1024\n",
    "TEST_BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# find out which device is available\n",
    "def device_type():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "DEVICE = torch.device(device_type())\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot use the whole dataset; it is too large for computers to handle. Instead, we perform *stochastic* gradient descent, i.e. we feed the model part of the data called batches. In order to do so, we use Pytorch DataLoaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the dataloader for the training dataset.\n",
    "# Here we shuffle the data to promote stochasticity.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "\n",
    "# Construct the dataloader for the testing dataset.\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the first 10 images of the train dataset.\n",
    "images = next(iter(train_dataloader))[0][:10]\n",
    "grid = torchvision.utils.make_grid(images, nrow=5, padding=10)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Now, we are ready to define our model. We will start with a simple model, a MultiLayer Perceptron (MLP) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # define the different modules of the network\n",
    "        super().__init__()\n",
    "        # How many features should our model have?\n",
    "        self.fc1 = nn.Linear(784, 50)\n",
    "\n",
    "        # How many outputs should our model have?\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        # we also define the non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # You should (a) transform the input to a size that is readable\n",
    "        # by the MLP and (b) pass the input x successively\n",
    "        # through the layers.\n",
    "        # ***************************************************\n",
    "        # transform the image to a vector\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # pass the vectored image through the layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Net()\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define:\n",
    "* the `fit` function that performs the training part\n",
    "* the `predict` function that takes as input the test dataloader and prints the performance metrics (e.g. accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    "    ):\n",
    "    '''\n",
    "    This function implements the core components of any Neural Network training regiment.\n",
    "    In our stochastic setting our code follows a very specific \"path\". First, we load a single batch and zero the optimizer. \n",
    "    Then we perform the forward pass, compute the gradients and perform the backward pass. And ...repeat!\n",
    "    '''\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        # move data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do the forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # perform the gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_dataloader.dataset)\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: torch.device):\n",
    "    '''\n",
    "    the fit method simply calls the train_epoch() method for a\n",
    "    specified number of epochs.\n",
    "    '''\n",
    "\n",
    "    # keep track of the losses in order to visualize them later\n",
    "    # Train for numerous epochs:\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = train_epoch(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"Epoch {epoch}: Loss={running_loss}\")\n",
    "        losses.append(running_loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: nn.Module, test_dataloader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
    "\n",
    "    print(f'Test set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_dataloader.dataset)} ({accuracy:.0f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a \"sanity check\". Our model is at the moment initialized randomly and we have 10 classes (each class has approximately the same number of samples). This means that we should get random performance -> ~10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 20 epochs\n",
    "losses = fit(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=20,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the loss progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss progression across epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very good. There are some major problems. We see from the plot above that the loss keeps dropping and does not \"plateau\". This indicates that we can run the optimization a few more epochs and improve the performance. Another point is that our learning rate is too slow or the selection of vanilla SGD as our optimizer is not optimal. In the next section we will see that simply changing the optimizer (from SGD to Adam) yields very different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: CNN\n",
    "\n",
    "## Evaluated Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the MLP does not take into account the nature of images: close pixels convey local information that is important. Using an MLP, we do not have the notion of the \"pixel neighbourhood\". Therefore, we neglect important information with an MLP. However, there are models better suited for vision problems, such as Convolutional Neural Networks or CNNs.\n",
    "\n",
    "With the code structure we have created, we can simply define a CNN and test its performance quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ************ YOUR CODE HERE ************\n",
    "        # define a CNN with 2 convolutional layers, followed by ReLU and Maxpool each,\n",
    "        # and a fully connected layer at the end.\n",
    "        # Hint: you could use nn.Sequential() for the convolutional part\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ************ YOUR CODE HERE ************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "cnn = CNN().to(DEVICE)\n",
    "\n",
    "# define the optimizer. Use Adam\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train the CNN\n",
    "losses = fit(\n",
    "    model=cnn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=15,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the CNN perform compared to the MLP?\n",
    "predict(model=cnn, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Loss progression across epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: To go further: play with CIFAR10\n",
    "\n",
    "MNIST is a fairly simple dataset. What happens in more challenging datasets? Try to train a network on CIFAR10 dataset and see how it performs. You can use the same code as above, but you need to change the model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foli25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
