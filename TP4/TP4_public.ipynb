{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IuwPrRsQfgrL"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImZRyj9kfi9N"
      },
      "source": [
        "# What you will learn today:\n",
        "We will introduce **scikit-learn**, one of the most important tool to know in Machine Learning. We will see different cool features that allow us to tackle machine learning problems in an easily-codable way. \n",
        "\n",
        "We will implement KNN (that we saw in the last exercise) with scikit-learn and  see that it will be just few lin of codes. We will look at the train and test error as a function of the \"complexity\" of the model, we will discuss the fundamental Bias-Variance trade-off.\n",
        "Finally we introduce then the Ensemble methods (Bagging and Boosting) and we analyze the performance of random forest in detail. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoRAEQ26OJuD"
      },
      "source": [
        "# Introduction to Scikit-Learn: Machine Learning with Python\n",
        "\n",
        "This section was put together by following the amazing work of [Jake Vanderplas](http://www.vanderplas.com). Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_tutorial/).</i></small>\n",
        "\n",
        "This session will cover the basics of Scikit-Learn, a popular package containing a collection of tools for machine learning written in Python. See more at http://scikit-learn.org."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwPsxI-eOJuF"
      },
      "source": [
        "## About Scikit-Learn\n",
        "\n",
        "[Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to **well-known** machine learning algorithms within Python code, through a **clean, well-thought-out API**. It has been built by hundreds of contributors from around the world, and is used across industry and academia.\n",
        "\n",
        "Scikit-Learn is built upon Python's [NumPy (Numerical Python)](http://numpy.org) and [SciPy (Scientific Python)](http://scipy.org) libraries, which enable efficient in-core numerical and scientific computation within Python. As such, scikit-learn is not specifically designed for extremely large datasets, though there is [some work](https://github.com/ogrisel/parallel_ml_tutorial) in this area.\n",
        "\n",
        "For this short introduction, I'm going to stick to questions of in-core processing of small to medium datasets with Scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lArlDtF8OJuK"
      },
      "source": [
        "## Representation of Data in Scikit-learn\n",
        "\n",
        "Machine learning is about creating models from data: for that reason, we'll start by\n",
        "discussing how data can be represented in order to be understood by the computer.  Along\n",
        "with this, we'll build on our matplotlib examples from the previous section and show some\n",
        "examples of how to visualize data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgl08qqlOJuK"
      },
      "source": [
        "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
        "**two-dimensional array or matrix**.  The arrays can be\n",
        "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
        "The size of the array is expected to be `[n_samples, n_features]`\n",
        "\n",
        "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
        "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
        "  a row in database or CSV file,\n",
        "  or whatever you can describe with a fixed set of quantitative traits.\n",
        "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
        "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
        "  discrete-valued in some cases.\n",
        "\n",
        "The number of features must be fixed in advance. However it can be very high dimensional\n",
        "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
        "where `scipy.sparse` matrices can be useful, in that they are\n",
        "much more memory-efficient than numpy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYO_GfJ8LW1j"
      },
      "source": [
        "# Basic Principles of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bsk3_GjLW1j"
      },
      "source": [
        "Here we'll dive into the basic principles of machine learning, and how to\n",
        "utilize them via the Scikit-Learn API.\n",
        "\n",
        "After briefly introducing scikit-learn's *Estimator* object, we'll cover **supervised learning**, including *classification* and *regression* problems, and **unsupervised learning**, including *dimensinoality reduction* and *clustering* problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VwLhHa3LW1k"
      },
      "source": [
        "## The Scikit-learn Estimator Object\n",
        "\n",
        "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. For instance a linear regression is implemented as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UmwPC_v9LW1k"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11yHnkCLW1k"
      },
      "source": [
        "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT1kDQ-aLW1l",
        "outputId": "081fadc1-41c4-4e75-cd15-a931fbf3999c"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MeC1km7LW1l"
      },
      "source": [
        "**Estimated Model parameters**: When data is *fit* with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BGzIT_HNLW1l"
      },
      "outputs": [],
      "source": [
        "x = np.arange(10)\n",
        "y = 2 * x + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aadfRHMQLW1l",
        "outputId": "c0f14c79-4204-476a-b9bc-19b23d3c3e93"
      },
      "outputs": [],
      "source": [
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "fbIkCecLLW1m",
        "outputId": "95f882f6-a664-438c-e83e-3ab3517f9b6e"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, 'o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uygwPpXKLW1n",
        "outputId": "39f73ff3-6bde-444e-a76e-9e0f7d1bd252"
      },
      "outputs": [],
      "source": [
        "# The input data for sklearn is 2D: (samples == 10 x features == 1)\n",
        "X = x[:, np.newaxis]\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuVc3j9oLW1n",
        "outputId": "d718455a-25ab-4948-b04e-c6ba0afc1a6c"
      },
      "outputs": [],
      "source": [
        "# fit the model on our data\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAzC6IViLW1n",
        "outputId": "08e603d9-d527-48c2-cf24-98db49d9eb63"
      },
      "outputs": [],
      "source": [
        "# underscore at the end indicates a fit parameter\n",
        "print(model.coef_)\n",
        "print(model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avuiNBWVLW1n"
      },
      "source": [
        "The model found a line with a slope 2 and intercept 1, as we'd expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvzCnxj9LW1n"
      },
      "source": [
        "## Supervised Learning: Classification and Regression\n",
        "\n",
        "In **Supervised Learning**, we have a dataset consisting of both features and labels.\n",
        "The task is to construct an estimator which is able to predict the label of an object\n",
        "given the set of features. A relatively simple example is predicting the species of \n",
        "iris given a set of measurements of its flower. This is a relatively simple task. \n",
        "Some more complicated examples are:\n",
        "\n",
        "- given a multicolor image of an object through a telescope, determine\n",
        "  whether that object is a star, a quasar, or a galaxy.\n",
        "- given a photograph of a person, identify the person in the photo.\n",
        "- given a list of movies a person has watched and their personal rating\n",
        "  of the movie, recommend a list of movies they would like\n",
        "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
        "\n",
        "What these tasks have in common is that there is one or more unknown\n",
        "quantities associated with the object which needs to be determined from other\n",
        "observed quantities.\n",
        "\n",
        "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
        "In classification, the label is discrete, while in regression, the label is continuous. For example,\n",
        "in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a\n",
        "classification problem: the label is from three distinct categories. On the other hand, we might\n",
        "wish to estimate the age of an object based on such observations: this would be a regression problem,\n",
        "because the label (age) is a continuous quantity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXoWT7qLW1p"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_FPgrXLW1p"
      },
      "source": [
        "### Regression Example\n",
        "\n",
        "One of the simplest regression problems is fitting a line to data, which we saw above.\n",
        "Scikit-learn also contains more sophisticated regression algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "wKjmaMLcLW1p",
        "outputId": "bd961ee8-c8db-4f03-e221-9b13300dede5"
      },
      "outputs": [],
      "source": [
        "# Create some simple data\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "X = np.random.random(size=(20, 1))\n",
        "y = 3 * X.squeeze() + 2 + np.random.randn(20)\n",
        "\n",
        "plt.plot(X.squeeze(), y, 'o');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi1SRNvxLW1p"
      },
      "source": [
        "As above, we can plot a line of best fit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "96AeRqjjLW1p",
        "outputId": "335ed7db-03ef-4e1f-fe9d-3c13cea990b2"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot the data and the model prediction\n",
        "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
        "y_fit = model.predict(X_fit)\n",
        "\n",
        "plt.plot(X.squeeze(), y, 'o')\n",
        "plt.plot(X_fit.squeeze(), y_fit);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdT9WnJLW1q"
      },
      "source": [
        "Scikit-learn also has some more sophisticated models, which can respond to finer features in the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "k8_UwGEWLW1q",
        "outputId": "bd24cf8f-7934-4a0c-9f5b-9dd6af87c95b"
      },
      "outputs": [],
      "source": [
        "# Fit a Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot the data and the model prediction\n",
        "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
        "y_fit = model.predict(X_fit)\n",
        "\n",
        "plt.plot(X.squeeze(), y, 'o')\n",
        "plt.plot(X_fit.squeeze(), y_fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdmbtNcLW1q"
      },
      "source": [
        "Whether either of these is a \"good\" fit or not depends on a number of things; we'll discuss details of how to choose a model later in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zgwG9NmdEqa"
      },
      "source": [
        "**Classification example**\n",
        "\n",
        "Generate data from the GMM introduced in TP3 and try to implement your first classification algorithm using scikit-learn! No worries if you do not manage. The exercise has the main purpose to let you familiarize with the browsing on the internet and to understand how to tackle a practical task. \n",
        "\n",
        "\n",
        "*Hint*: Search for sklearn.neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekttmQB8yc1"
      },
      "source": [
        "So let's see how scikit-learn's k-NN implementation works. One nice thing about scikit-learn is that they have lots of examples available online, so we can just look for something similar to what we are trying to do. For k-NN classification in particular there's this: http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZQZvg-k8yc1",
        "outputId": "4ed69ff2-6778-4533-ede0-903175043bcb"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors\n",
        "clf = neighbors.KNeighborsClassifier(10)\n",
        "# Samples 10 centroids for each class from two different bivariate Normal distributions\n",
        "centroids_per_class = 10\n",
        "\n",
        "class0_centroids = [1, 0] + np.random.randn(centroids_per_class, 2)\n",
        "class1_centroids = [0, 1] + np.random.randn(centroids_per_class, 2)\n",
        "samples_per_class = 100\n",
        "\n",
        "# Sample actual data sampling from Normal distributions positioned around the centroids (Pick the centroid of the chosen class uniformly)\n",
        "class0_labels = np.random.randint(10, size = samples_per_class)\n",
        "class1_labels = np.random.randint(10, size = samples_per_class)\n",
        "\n",
        "class0_samples = class0_centroids[class0_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
        "class1_samples = class1_centroids[class1_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
        "\n",
        "# Plot data\n",
        "plt.figure(figsize=((12,5)))\n",
        "plt.plot(class0_samples[:, 0], class0_samples[:, 1], \"o\", markersize=8)\n",
        "plt.plot(class1_samples[:, 0], class1_samples[:, 1], \"x\", markersize=8)\n",
        "plt.title(\"Samples\")\n",
        "X = np.vstack((class0_samples, class1_samples))\n",
        "y = np.hstack((np.zeros(samples_per_class), np.ones(samples_per_class)))\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ShrU9p8yc1"
      },
      "source": [
        "And done! With just three lines of code we were able to repeat everything we've been doing so far.\n",
        "\n",
        " Let us see if we get the same training error as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample test data from the model\n",
        "testsamples_per_class = 10000\n",
        "\n",
        "class0_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
        "class1_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
        "class0_testsamples = class0_centroids[class0_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
        "class1_testsamples = class1_centroids[class1_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
        "\n",
        "X_test = np.vstack((class0_testsamples, class1_testsamples))\n",
        "y_test = np.hstack((np.zeros(testsamples_per_class), np.ones(testsamples_per_class)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D28FBUqE8yc1",
        "outputId": "2a10d04f-60c8-4f91-f76d-aa9dc57bcb9b"
      },
      "outputs": [],
      "source": [
        "train_error = np.mean(y != clf.predict(X))\n",
        "test_error = np.mean(y_test != clf.predict(X_test))\n",
        "\n",
        "print(\"train/test error (for k = 10): %g/%g\" % (train_error, test_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imDtIlwqzZwH"
      },
      "source": [
        "We could use also the *.score* method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zAVTjunzfRe",
        "outputId": "82b261e2-7a27-40f0-ecbc-fd1d548a48e2"
      },
      "outputs": [],
      "source": [
        "train_error = 1. - clf.score(X, y)\n",
        "test_error = 1. - clf.score(X_test, y_test)\n",
        "\n",
        "print(\"train/test error (for k = 10): %g/%g\" % (train_error, test_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCHpAU-n8yc1"
      },
      "source": [
        "Since things are optimized here, we can even do cool things such as plotting the actual decision boundaries. \n",
        "\n",
        "In order to do that, let us generate a grid and compute the estimate for each point in that grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGlys2zH8yc2",
        "outputId": "a64e926d-bd87-4601-f42f-c9c64d98ae51"
      },
      "outputs": [],
      "source": [
        "# Create a grid\n",
        "xx, yy = np.meshgrid(np.linspace(-5, 5, 101), np.linspace(-5, 5, 101))\n",
        "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Do some plotting\n",
        "zz = zz.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, zz, cmap = \"coolwarm\", alpha = 0.2)\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"o\", markersize=8)\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"x\", markersize=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brEU18FhzpTU"
      },
      "source": [
        "...scikit-learn is a beautiful piace of engineering!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we3TVyQDLW1r"
      },
      "source": [
        "### Recap: Scikit-learn's estimator interface\n",
        "\n",
        "Scikit-learn strives to have a uniform interface across all methods,\n",
        "and we'll see examples of these below. Given a scikit-learn *estimator*\n",
        "object named `model`, the following methods are available:\n",
        "\n",
        "- Available in **all Estimators**\n",
        "  + `model.fit()` : fit training data. For supervised learning applications,\n",
        "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
        "    For unsupervised learning applications, this accepts only a single argument,\n",
        "    the data `X` (e.g. `model.fit(X)`).\n",
        "- Available in **supervised estimators**\n",
        "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
        "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
        "    and returns the learned label for each object in the array.\n",
        "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
        "    this method, which returns the probability that a new observation has each categorical label.\n",
        "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
        "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
        "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
        "- Available in **unsupervised estimators**\n",
        "  + `model.predict()` : predict labels in clustering algorithms.\n",
        "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
        "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
        "    on the unsupervised model.\n",
        "  + `model.fit_transform()` : some estimators implement this method,\n",
        "    which more efficiently performs a fit and a transform on the same input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDUBgR7X8yc2"
      },
      "source": [
        "### Determining the best value of $k$ by cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnP5gene8yc2"
      },
      "source": [
        "Let us introduce a way to pinpoint the \"best\" value of $k$, called **cross-validation**.\n",
        "The main idea is to partition our dataset into two called *training* and *validation* sets. \n",
        "\n",
        "We can pinpoint the optimal $k$ so as to minimize the validation error.\n",
        "\n",
        "\n",
        "**Caveat**: We then need a 3rd set where we can compute the test error -- **NEVER** adjust your parameters and compute the test error using the same set!\n",
        "\n",
        "However, this does not provide us good statistics since the partitioning is only done once; a perhaps better way of doing it is by partitioning the dataset again and again, and recomputing the score. \n",
        "\n",
        "This is the idea behind cross-validation and scikit-learn has nice convenience functions to perform cross-validation for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Egr3U38yc2",
        "outputId": "bcac9fce-461c-4c30-9767-feb35e416ff6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set up the grid search\n",
        "parameters = [{'n_neighbors': np.arange(1, 20)}]\n",
        "clf = GridSearchCV(\n",
        "    neighbors.KNeighborsClassifier(),\n",
        "     parameters\n",
        "     )\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print results\n",
        "print(clf.best_params_)\n",
        "print(clf.cv_results_[\"mean_test_score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjuo2LNK8yc2"
      },
      "source": [
        "Let's see how this does on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6dYLxGh8yc2",
        "outputId": "0d4f4575-d068-4874-c472-850de38370dc"
      },
      "outputs": [],
      "source": [
        "train_error = np.mean(y != clf.predict(X))\n",
        "test_error = np.mean(y_test != clf.predict(X_test))\n",
        "print(\"train/test error (for optimal k): %g/%g\" % (train_error, test_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_xHzkccLW1s"
      },
      "source": [
        "## Flow Chart: How to Choose your Estimator\n",
        "\n",
        "[There](https://scikit-learn.org/stable/machine_learning_map.html) is a flow chart created by scikit-learn super-contributor [Andreas Mueller](https://github.com/amueller) which gives a nice summary of which algorithms to choose in various situations. Keep it around as a handy reference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIu4ETnzLW1t"
      },
      "source": [
        "Original source on the [scikit-learn website](http://scikit-learn.org/stable/tutorial/machine_learning_map/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BArd1qhiR9gJ"
      },
      "source": [
        "# Bias-Variance Trade-off \n",
        "\n",
        "We analyze in this section one key idea of Machine Learning: the Bias-Variance trade-off. First we see it with KNN, our dear friend by now. Then we go forward and we study the Random Forest case. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3VcVFV-ggtR"
      },
      "source": [
        "### KNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOlCs1OHgmcT"
      },
      "source": [
        "Let us rewrite briefly the code for generating the GMM dataset of TP3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "nBtX7pUu8ycq",
        "outputId": "4ca2e542-543d-496a-a3d7-d62435375d08"
      },
      "outputs": [],
      "source": [
        "# Samples 10 centroids for each class from two different bivariate Normal distributions\n",
        "centroids_per_class = 10\n",
        "\n",
        "class0_centroids = [1, 0] + np.random.randn(centroids_per_class, 2)\n",
        "class1_centroids = [0, 1] + np.random.randn(centroids_per_class, 2)\n",
        "samples_per_class = 100\n",
        "\n",
        "# Sample actual data sampling from Normal distributions positioned around the centroids (Pick the centroid of the chosen class uniformly)\n",
        "class0_labels = np.random.randint(10, size = samples_per_class)\n",
        "class1_labels = np.random.randint(10, size = samples_per_class)\n",
        "\n",
        "class0_samples = class0_centroids[class0_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
        "class1_samples = class1_centroids[class1_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
        "\n",
        "# Plot data\n",
        "plt.figure(figsize=((12,5)))\n",
        "plt.plot(class0_samples[:, 0], class0_samples[:, 1], \"o\", markersize=8)\n",
        "plt.plot(class1_samples[:, 0], class1_samples[:, 1], \"x\", markersize=8)\n",
        "plt.title(\"Samples\")\n",
        "X = np.vstack((class0_samples, class1_samples))\n",
        "y = np.hstack((np.zeros(samples_per_class), np.ones(samples_per_class)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYXAJFqT8ycx"
      },
      "outputs": [],
      "source": [
        "# Sample test data from the model\n",
        "testsamples_per_class = 10000\n",
        "\n",
        "class0_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
        "class1_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
        "class0_testsamples = class0_centroids[class0_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
        "class1_testsamples = class1_centroids[class1_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
        "\n",
        "X_test = np.vstack((class0_testsamples, class1_testsamples))\n",
        "y_test = np.hstack((np.zeros(testsamples_per_class), np.ones(testsamples_per_class)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NpVKKLeY8ycy",
        "outputId": "6ff53422-7d24-4996-96fd-6f1a33429e44"
      },
      "outputs": [],
      "source": [
        "# Plot test data\n",
        "plt.plot(X_test[y_test == 0, 0], X_test[y_test == 0, 1], \"o\", ms=8)\n",
        "plt.plot(X_test[y_test == 1, 0], X_test[y_test == 1, 1], \"x\", ms=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39loTOE5e7NC"
      },
      "source": [
        "We can write easily a function that computes the train and test error of KNN using sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzNpbwIa8ycy"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors\n",
        "def knn_errors(X_train, y_train, X_test, y_test, k):\n",
        "    n_train, n_test = len(y_train), len(y_test)\n",
        "    clf = neighbors.KNeighborsClassifier(k)\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    train_error = np.mean(y != clf.predict(X))\n",
        "    test_error = np.mean(y_test != clf.predict(X_test))\n",
        "\n",
        "    # Or use the score method \n",
        "    train_error = 1. - clf.score(X, y)\n",
        "    test_error = 1. - clf.score(X_test, y_test)\n",
        "\n",
        "    return train_error,test_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1SjvcJL8ycz"
      },
      "source": [
        "It is instructive to look at the dependence of the error as a function of another quantity instead of $k$: the number of **degrees of freedom** $N / k$. \n",
        "\n",
        "Indeed, the larger the $k$, the smaller the number of effective parameters -- think for instance of the $k = N$ limit, where everyone is assigned the same label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "X2rGiTqh8yc0",
        "outputId": "1a7ec485-73d3-4bd9-ab1a-54e2a1c53621"
      },
      "outputs": [],
      "source": [
        "# Re-run the experiment above for a different range of values\n",
        "ks = np.r_[np.arange(1, 10), np.arange(10, 150, 30)]\n",
        "train_error = []\n",
        "test_error = []\n",
        "for k in ks:\n",
        "    knn_errs = knn_errors(X, y, X_test, y_test,k)\n",
        "    train_error.append(knn_errs[0])\n",
        "    test_error.append(knn_errs[1])\n",
        "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error[-1], test_error[-1]))\n",
        "\n",
        "# Plot error as a function of the degrees of freedom\n",
        "plt.plot(len(y) / np.array(ks), train_error, \"-o\", label = \"train\")\n",
        "plt.plot(len(y) / np.array(ks), test_error, \"-o\", label = \"test\")\n",
        "plt.legend()\n",
        "plt.xlabel(r\"degrees of freedom $N / k$\")\n",
        "plt.ylabel(\"misclassification error\")\n",
        "#plt.ylim((0.025, 0.15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XiDP4FY8yc0"
      },
      "source": [
        "This plot makes explicit the so-called **bias-variance tradeoff** that appears all throughout statistics. If we have more parameters, we are able to get smaller training error, but the test (analogously, generalization) error actually increases, meaning we are overfitting! \n",
        "\n",
        "Remember the pivotal difference between learning and memorizing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA9uDSEQfgrO"
      },
      "source": [
        "# Linear regression on the Ozone dataset - Regression trees and random forest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypQfL7SfgrP"
      },
      "source": [
        "## The Ozone dataset\n",
        "\n",
        "Let's take a look at a particular real-life data problem: the prediction of ozone concentration as a factor of other weather-based features. As with all data problems, it behooves us to take a look at all of the information that we have about the dataset.\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "These data record the level of atmospheric ozone concentration from\n",
        "eight daily meteorological measurements made in the Los Angeles basin\n",
        "in 1976.  Although measurements were made every day that year, some\n",
        "observations were missing; here we have the 330 complete cases.  The\n",
        "data were given to us by Leo Breiman; he was a consultant on a project\n",
        "from which these data are taken.  The response, referred to as ozone,\n",
        "is actually the log of the daily maximum of the hourly-average ozone\n",
        "concentrations in Upland, California.\n",
        "\n",
        "Source: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/LAozone.data\n",
        "\n",
        "Detailed variable names:\n",
        "\n",
        "- ozone : Upland Maximum Ozone\n",
        "- vh : Vandenberg 500 mb Height\n",
        "- wind : Wind Speed (mph)\n",
        "- humidity : Humidity (%)\n",
        "- temp : Sandburg AFB Temperature\n",
        "- ibh : Inversion Base Height\n",
        "- dpg : Daggot Pressure Gradient\n",
        "- ibt : Inversion Base Temperature\n",
        "- vis : Visibility (miles)\n",
        "- doy : Day of the Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7jtjJ1AfgrQ"
      },
      "source": [
        "Now let's take a look at what this dataset looks like. We have imported the datased into a pandas dataframe. See e.g. http://pythonhow.com/accessing-dataframe-columns-rows-and-cells/ for a short tutorial on dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU3iiUYIfgrQ",
        "outputId": "1ba02445-3fac-4726-e5e9-8d3969fc566f"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/LAozone.data')\n",
        "print(f\"data: {data.shape[0]} rows and {data.shape[1]} columns\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlOvILIfgrR"
      },
      "source": [
        "Alright, we're ready to get started ! Our dataset contains\n",
        "\n",
        "$\\bullet$ $N=330$ data points $x_i$, each containing $P=9$ meteorological features \n",
        "\n",
        "$\\bullet$ one target $y_i$, the Ozone concentration\n",
        "\n",
        "Now, before we touch anything, we need to follow best practices. When faced with a new dataset, we need to set up some kind of objective comparison. To do this, we need to split our dataset into three parts: **Training** (and within that, **Validation**), and **Testing** sets. \n",
        "\n",
        "The best practice here is to take the test data and lock it away somewhere. It is always tempting to tune your algorithms to give the best test performance. However, even if the regression isn't explicitly *trained* on the test data, as practitioners, we could be continually making changes in an effort to get our numbers up.\n",
        "\n",
        "Instead, we should deep-freeze the test data, and then tune as much as we can via **cross-validation (CV)** on our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "045j_eYRfgrR",
        "outputId": "46d2c93f-6301-468c-beb9-e8a4b76abb50"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Convert from DataFrame to array\n",
        "y = data['ozone'].values\n",
        "X = data.iloc[:,1:10].astype(float).values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=1)\n",
        "\n",
        "print(\"Training samples: \", len(y_train))\n",
        "print(\"Testing samples: \", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9706X2TfgrS"
      },
      "source": [
        "Now, before we start attempting to fit models, lets take a bit of care and apply some pre-processing to our dataset. The de-facto pre-processing is *centering and normalization*. Specifically, many flavors of estimators (OLS, RR, etc.) can be thrown of by large differences in of scale and variations between the features. We can easily account for this in our estimators by simply normalizing the feature columns and removing averages. Scikit-Learn has some features for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-Ypqq35TfgrS"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing \n",
        "\n",
        "# Center and scale features and observations\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "ymean = np.mean(y_train)\n",
        "y_train = y_train - ymean\n",
        "y_test = y_test - ymean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCky7VxfgrT"
      },
      "source": [
        "Now it is time for us to choose our estimator. What should we choose? \n",
        "\n",
        "We will introduce **Regression Trees**, and on them we will apply the methods of **Bagging**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dwwoUbUfgrX"
      },
      "source": [
        "# Evaluated Question \n",
        "## Regression trees\n",
        "\n",
        "Now, we introduce regression trees. Regression trees are perhaps the simplest non-linear regression model one can imagine. The tree is constructed recursively. At each step, the training set is split in two parts according to a binary question (\"Is feature $x_\\mu < q$?\"). The feature and the threshold are chosen to minimize a Loss function, e.g. Mean Square Error (MSE) or Mean Absolute Error (MAE).\n",
        "\n",
        "Once the tree is constructed, for a new sample $x_{new}$, one simply determines in which leave it falls, and then assign the value $y_{new}$ as the mean (for MSE) or the median (for MAE) of the traning samples in that leave.\n",
        "\n",
        "**Exercise**: Explore the Regression tree model implemented in scikit-learn. Regression trees (e.g. as implemented in sklearn) have many free parameters: the maximum depth of the tree, the minimum number of samples that should be contained in each split, the minimum number of samples in the leaves, etc. Perform CV for at least some of these parameters and discuss your results. \n",
        "\n",
        "See http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor for documentation, and references therein for more details on regression trees.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkDY24rQfgrY"
      },
      "source": [
        "# BONUS SECTION: Ensemble methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB_vZ8bXfgrY"
      },
      "source": [
        "The main idea behind ensemble methods is to improve multiple poor models from the data at hand, by combining them in a clever way. We are thus going to use the regression tree as the basic models, as we have seen that it provides quite poor results, and try to improve by considering ensembles of trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD1Afrg5fgrZ"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJqlx0VRfgrZ"
      },
      "source": [
        "In bagging, we generate multiple models *in parallel*, each of them working with a different dataset, sampled at random from the original training set (i.e. we do *bootstrapping*). When sampling, we consider a different random subset of features $P'<P$ for each dataset, and we sample with replacement $N$ data points.\n",
        "\n",
        "When we do that with decision trees, we get the **random forests** algorithm.\n",
        "A good tutorial can be found here\n",
        "https://gormanalysis.com/random-forest-from-top-to-bottom/\n",
        "\n",
        "In principle, also in the random forests algorithm we have many free parameters (max split, max depth, etc.) but we will let the algorithm find the best trees, without performing cross validation in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBs6jk_hA5A"
      },
      "source": [
        "**Your turn**\n",
        "\n",
        "Implement Random Forest Regression on the ozone dataset. Look for sklearn.ensemble.RandomForestRegressor! Again, no worries if you do not manage at the first try, the scope of this exercise is to learn how to move in the sklearn environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
